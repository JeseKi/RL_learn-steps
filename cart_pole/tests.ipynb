{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc98a1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练 1000 个回合...\n",
      "Episode   10: Average Reward (last 10):  486.1, Epsilon: 0.017\n",
      "Episode   20: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode   30: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode   40: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode   50: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode   60: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode   70: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode   80: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode   90: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  100: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  110: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  120: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  130: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  140: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  150: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  160: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  170: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  180: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  190: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  200: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  210: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  220: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  230: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  240: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  250: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  260: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  270: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  280: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  290: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  300: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  310: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  320: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  330: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  340: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  350: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  360: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  370: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  380: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  390: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  400: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  410: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  420: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  430: Average Reward (last 10):  486.1, Epsilon: 0.010\n",
      "Episode  440: Average Reward (last 10):  486.1, Epsilon: 0.010\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m env \u001b[38;5;241m=\u001b[39m CartPole(SEED, cfg)\n\u001b[1;32m     11\u001b[0m agent \u001b[38;5;241m=\u001b[39m LinearQNet(SEED, agent_cfg)\n\u001b[0;32m---> 13\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRewards:\u001b[39m\u001b[38;5;124m\"\u001b[39m, rewards)\n",
      "File \u001b[0;32m~/Projects/learn/Python/rl_atomic/cart_pole/train.py:52\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env, agent, cfg, num_episodes, log_interval)\u001b[0m\n\u001b[1;32m     49\u001b[0m step_result \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action_result\u001b[38;5;241m.\u001b[39mbest_action)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# 5. TD更新\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_td0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43ms_next\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdone\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# 6. 更新状态和奖励\u001b[39;00m\n\u001b[1;32m     61\u001b[0m state \u001b[38;5;241m=\u001b[39m step_result\u001b[38;5;241m.\u001b[39mstate\n",
      "File \u001b[0;32m~/Projects/learn/Python/rl_atomic/cart_pole/core.py:152\u001b[0m, in \u001b[0;36mLinearQNet.update_td0\u001b[0;34m(self, s, a, r, s_next, done)\u001b[0m\n\u001b[1;32m    149\u001b[0m td_error \u001b[38;5;241m=\u001b[39m target_q \u001b[38;5;241m-\u001b[39m q_s_a\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# 6. 提取特征向量\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m phi_arr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mphi_s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphi_s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphi_s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_dot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphi_s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_dot2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mphi_s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphi_s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtheta2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphi_s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtheta_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mphi_s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtheta_dot2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphi_s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# 7. 更新权重矩阵对应行\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# ∇Wᵢ = φ(s) * td_error (对于执行的动作i)\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Wᵢ ← Wᵢ + α * td_error * φ(s)\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW[action_idx] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m td_error \u001b[38;5;241m*\u001b[39m phi_arr\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from core import CartPole, LinearQNet\n",
    "from schemas import CartPoleConfig, AgentConfig\n",
    "from train import train\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "cfg = CartPoleConfig()\n",
    "agent_cfg = AgentConfig(eps_rate=0.999)\n",
    "\n",
    "env = CartPole(SEED, cfg)\n",
    "agent = LinearQNet(SEED, agent_cfg)\n",
    "\n",
    "rewards = train(env=env, agent=agent, cfg=cfg, num_episodes=1000, log_interval=10)\n",
    "\n",
    "print(\"Training completed.\")\n",
    "print(\"Rewards:\", rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a8c17a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
