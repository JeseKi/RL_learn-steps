{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e3bcc5",
   "metadata": {},
   "source": [
    "# 贪婪算法的不同和优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca6aed0",
   "metadata": {},
   "source": [
    "## 现在将创建不同的 Agent 并统计平均值，后悔值，命中率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8b76cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Tuple\n",
    "\n",
    "from core import EpsilonDecreasingConfig, GreedyAgent, Rewards, RLEnv\n",
    "from train import train, AverageMetrics\n",
    "from algorithms import greedy_average, epsilon_average, epsilon_decreasing_average\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7795a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "env = RLEnv(machine_count=100, seed=SEED)\n",
    "COUNT = 50\n",
    "STEPS = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_train(\n",
    "    count: int,\n",
    "    greedy_algorithm: Callable[..., int],\n",
    "    env: RLEnv,\n",
    "    epsilon_config: EpsilonDecreasingConfig,\n",
    "    steps: int,\n",
    "    optimistic_init: bool,\n",
    "    optimistic_times: int,\n",
    "    seed: int,\n",
    ") -> Tuple[List[GreedyAgent], Rewards, AverageMetrics]:\n",
    "    \"\"\"批训练 Agent，传入数量，不同的算法，环境，步数和初始种子即可训练\n",
    "\n",
    "    Args:\n",
    "        count (int): 训练数量\n",
    "        agent (GreedyAgent): 算法 类型\n",
    "        env (RLEnv): 环境\n",
    "        steps (int): 步数\n",
    "        seed (int): 初始种子\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[GreedyAgent], Rewards]: 返回训练后的 agents 和平均后的奖励\n",
    "    \"\"\"\n",
    "    _agents: List[GreedyAgent] = []\n",
    "\n",
    "    if not callable(greedy_algorithm):\n",
    "        raise ValueError(\"算法必须传入一个函数\")\n",
    "\n",
    "    for i in range(count):\n",
    "        _agents.append(\n",
    "            GreedyAgent(\n",
    "                name=greedy_algorithm.__name__,  # type: ignore # 在 callable 这里就已经验证了是一个函数，这里是为了避免 ty 工具误报\n",
    "                env=env,\n",
    "                greedy_algorithm=greedy_algorithm,\n",
    "                epsilon_config=epsilon_config,\n",
    "                optimistic_init=optimistic_init,\n",
    "                optimistic_times=optimistic_times,\n",
    "                seed=seed + i,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    agents, reward, metrics = train(_agents, steps)\n",
    "\n",
    "    return agents, reward, metrics\n",
    "\n",
    "\n",
    "def plot_metrics_history(agents: List[GreedyAgent], agent_name: str):\n",
    "    \"\"\"\n",
    "    根据训练后的一组 agent 的 metrics_history 绘制指标变化图。\n",
    "\n",
    "    Args:\n",
    "        agents (List[GreedyAgent]): 经过训练的 agent 列表。\n",
    "        agent_name (str): 这组 agent 的名称，用于图表标题。\n",
    "    \"\"\"\n",
    "\n",
    "    if not agents:\n",
    "        raise ValueError(\"Agents 列表为空，无法绘图\")\n",
    "\n",
    "    # 1. 设置字体\n",
    "    font_path = Path.cwd() / \"assets\" / \"微软雅黑.ttf\"\n",
    "    if font_path.exists():\n",
    "        font_prop = FontProperties(fname=font_path, size=12)\n",
    "        title_font_prop = FontProperties(fname=font_path, size=16)\n",
    "        print(\"成功加载字体：\", font_path)\n",
    "\n",
    "    else:\n",
    "        print(f\"警告：找不到字体文件 {font_path}，将使用默认字体，中文可能显示为方框。\")\n",
    "        font_prop = FontProperties(size=12)\n",
    "        title_font_prop = FontProperties(size=16)\n",
    "\n",
    "    # 2. 准备数据\n",
    "    num_steps: int = agents[0].steps\n",
    "\n",
    "    avg_history = {\n",
    "        \"regret\": np.zeros(num_steps),\n",
    "        \"regret_rate\": np.zeros(num_steps),\n",
    "        \"total_reward\": np.zeros(num_steps),\n",
    "        \"optimal_rate\": np.zeros(num_steps),\n",
    "    }\n",
    "\n",
    "    # 遍历每个时间步\n",
    "    for step_idx in range(num_steps):\n",
    "        # 临时存储当前时间步所有 agent 的指标\n",
    "        step_metrics = {\n",
    "            \"regret\": [],\n",
    "            \"regret_rate\": [],\n",
    "            \"total_reward\": [],\n",
    "            \"optimal_rate\": [],\n",
    "        }\n",
    "        # 遍历每个 agent\n",
    "        for agent in agents:\n",
    "            # agent.metrics_history 的索引与 step_idx 一致\n",
    "            if step_idx < len(agent.metrics_history):\n",
    "                metrics_at_step = agent.metrics_history[step_idx][\n",
    "                    1\n",
    "                ]  # (Rewards, Metrics, int) -> Metrics\n",
    "                step_metrics[\"regret\"].append(metrics_at_step.regret)\n",
    "                step_metrics[\"regret_rate\"].append(metrics_at_step.regret_rate)\n",
    "                step_metrics[\"total_reward\"].append(sum(metrics_at_step.rewards.values))\n",
    "                step_metrics[\"optimal_rate\"].append(metrics_at_step.optimal_rate)\n",
    "\n",
    "        # 计算当前时间步的平均值并存入 avg_history\n",
    "        for key in avg_history:\n",
    "            if step_metrics[key]:  # 确保列表不为空\n",
    "                avg_history[key][step_idx] = np.mean(step_metrics[key])\n",
    "\n",
    "    steps_axis = np.arange(1, num_steps + 1)\n",
    "\n",
    "    # 3. 开始绘图\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12), dpi=100)\n",
    "    fig.suptitle(f'\"{agent_name}\" 算法平均指标变化情况', fontproperties=title_font_prop)\n",
    "\n",
    "    assert isinstance(axes, np.ndarray)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    plot_config = {\n",
    "        \"regret\": \"后悔值 (Regret)\",\n",
    "        \"regret_rate\": \"后悔率 (Regret Rate)\",\n",
    "        \"total_reward\": \"累积总奖励 (Total Reward)\",\n",
    "        \"optimal_rate\": \"最优臂选择率 (Optimal Rate)\",\n",
    "    }\n",
    "\n",
    "    for i, (metric_key, title) in enumerate(plot_config.items()):\n",
    "        ax = axes[i]\n",
    "        ax.plot(steps_axis, avg_history[metric_key], label=title)\n",
    "        ax.set_title(title, fontproperties=font_prop)\n",
    "        ax.set_xlabel(\"时间步 (Steps)\", fontproperties=font_prop)\n",
    "        ax.set_ylabel(\"平均值\", fontproperties=font_prop)\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "        ax.legend(prop=font_prop)\n",
    "\n",
    "    plt.tight_layout(rect=(0, 0, 1, 0.96))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e98076",
   "metadata": {},
   "source": [
    "## 平均奖励"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c2c88b",
   "metadata": {},
   "source": [
    "### 普通贪婪算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d766b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 普通贪婪算法的结果\n",
    "agents, reward, metrics = batch_train(\n",
    "    count=COUNT,\n",
    "    greedy_algorithm=greedy_average,\n",
    "    env=env,\n",
    "    epsilon_config=EpsilonDecreasingConfig(),\n",
    "    steps=STEPS,\n",
    "    optimistic_init=True,\n",
    "    optimistic_times=5,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "plot_metrics_history(agents=agents, agent_name=\"普通贪婪算法\")\n",
    "\n",
    "print(f\"anget 名称: {agents[0].name}\\n平均奖励：{reward}\\n指标：{metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3ccd67",
   "metadata": {},
   "source": [
    "### 随机探索贪婪算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ffe04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机探索贪婪算法的结果\n",
    "agents, reward, metrics = batch_train(\n",
    "    count=COUNT,\n",
    "    greedy_algorithm=epsilon_average,\n",
    "    env=env,\n",
    "    epsilon_config=EpsilonDecreasingConfig(),\n",
    "    steps=STEPS,\n",
    "    optimistic_init=True,\n",
    "    optimistic_times=5,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "plot_metrics_history(agents=agents, agent_name=\"随机探索贪婪算法\")\n",
    "\n",
    "print(f\"anget 名称: {agents[0].name}\\n平均奖励：{reward}\\n指标：{metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efc5294",
   "metadata": {},
   "source": [
    "### 退火随机探索贪婪算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e480b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents, reward, metrics = batch_train(\n",
    "    count=COUNT,\n",
    "    greedy_algorithm=epsilon_decreasing_average,\n",
    "    env=env,\n",
    "    epsilon_config=EpsilonDecreasingConfig(),\n",
    "    steps=STEPS,\n",
    "    optimistic_init=True,\n",
    "    optimistic_times=5,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "plot_metrics_history(agents=agents, agent_name=\"退火随机探索贪婪算法\")\n",
    "\n",
    "print(f\"anget 名称: {agents[0].name}\\n平均奖励：{reward}\\n指标：{metrics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
