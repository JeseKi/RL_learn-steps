{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41b40c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any\n",
    "from pydantic import BaseModel, Field\n",
    "import math\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class CartPoleMini:\n",
    "    \"\"\"Cart Pole 极简版的自行实现\"\"\"\n",
    "\n",
    "    def __init__(self, seed, max_steps=5000) -> None:\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed) -> Any:\n",
    "        pass\n",
    "\n",
    "    def step(self, action) -> Any:\n",
    "        pass\n",
    "\n",
    "\n",
    "class State(BaseModel):\n",
    "    x: float = Field(default=0, description=\"小车位置\")\n",
    "    x_dot: float = Field(default=0, description=\"小车速度\")\n",
    "    theta: float = Field(default=0, description=\"杆子角度\")\n",
    "    theta_dot: float = Field(default=0, description=\"杆子角速度\")\n",
    "\n",
    "\n",
    "class Feature(BaseModel):\n",
    "    x: float = Field(default=0, description=\"小车位置\")\n",
    "    x2: float = Field(default=0, description=\"小车位置平方\")\n",
    "    x_dot: float = Field(default=0, description=\"小车速度\")\n",
    "    x_dot2: float = Field(default=0, description=\"小车速度平方\")\n",
    "    theta: float = Field(default=0, description=\"杆子角度\")\n",
    "    theta2: float = Field(default=0, description=\"杆子角度平方\")\n",
    "    theta_dot: float = Field(default=0, description=\"杆子角速度\")\n",
    "    theta_dot2: float = Field(default=0, description=\"杆子角速度平方\")\n",
    "    bias: float = Field(default=1.0, description=\"偏置项\")\n",
    "\n",
    "\n",
    "class Step(BaseModel):\n",
    "    state: State\n",
    "    reward: float = Field(default=1.0, description=\"奖励\")\n",
    "    done: bool = Field(default=False, description=\"是否结束，真正的任务完成/失败\")\n",
    "    terminated: bool = Field(\n",
    "        default=False,\n",
    "        description=\"是否终止，比如达到目标，或游戏失败，可能进行重置或下一局等\",\n",
    "    )\n",
    "    truncated: bool = Field(\n",
    "        default=False, description=\"是否截断，人为截断，比如达到最大步数，时间限制等\"\n",
    "    )\n",
    "    info: dict = Field(default_factory=dict, description=\"额外信息\")\n",
    "\n",
    "\n",
    "class Action(str, Enum):\n",
    "    LEFT = \"left\"\n",
    "    RIGHT = \"right\"\n",
    "\n",
    "\n",
    "class CartPoleConfig(BaseModel):\n",
    "    state_dim: int = Field(default=len(State().model_dump()), description=\"状态维度\")\n",
    "    phi_dim: int = Field(\n",
    "        default=len(Feature().model_dump()), description=\"手工特征维度\"\n",
    "    )\n",
    "    n_actions: int = Field(default=len(Action), description=\"动作个数\")\n",
    "    alpha: float = Field(default=0.01, description=\"学习率\")\n",
    "    gamma: float = Field(default=0.99, description=\"折扣因子，未来奖励的当前价值\")\n",
    "    eps_start: float = Field(\n",
    "        default=1.0,\n",
    "        description=\"epsilon-贪婪策略的初始epsilon，初期高探索，快速了解环境和哪些动作好\",\n",
    "    )\n",
    "    eps_end: float = Field(\n",
    "        default=0.01,\n",
    "        description=\"epsilon-贪婪策略的最终epsilon值，后期低探索，更多利用已学知识\",\n",
    "    )\n",
    "    eps_steps: int = Field(\n",
    "        default=5000,\n",
    "        description=\"epsilon-贪婪策略的epsilon衰减步数，  从eps_start 线性衰减到 eps_end 需要多少步\",\n",
    "    )\n",
    "    bias: float = Field(default=1.0, description=\"手工特征中的偏置项\")\n",
    "\n",
    "\n",
    "class LinearQNet:\n",
    "    \"\"\"\n",
    "    Q(s,a) = w_a^T * phi(s)  （phi: 特征映射）\n",
    "    这里使用简单的手工特征： [s, s^2, 1] 以提高表达力，但仍保持超小型。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: CartPoleConfig, seed=42) -> None:\n",
    "        pass\n",
    "\n",
    "    def phi(self, s) -> np.ndarray | Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def epsilon(self) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def q_values(self, s) -> np.ndarray | Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def act(self, s) -> Action:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def update_td0(self, s, a, r, s_, done):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec59cda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleMini(CartPoleMini):\n",
    "    def __init__(self, seed: int, max_steps=5000) -> None:\n",
    "        super().__init__(seed, max_steps)\n",
    "        self.gravity = 9.8  # 重力加速度\n",
    "        self.cart_mass = 1.0  # 小车质量\n",
    "        self.pole_mass = 0.1  # 杆子质量\n",
    "        self.total_mass = self.cart_mass + self.pole_mass  # 总质量\n",
    "        self.pole_length = 0.5  # 杆子长度的一半\n",
    "        self.pole_mass_length = (\n",
    "            self.pole_mass * self.pole_length\n",
    "        )  # 杆子质量与长度的乘积\n",
    "        self.force_mag = 1  # 作用在小车上的力的大小\n",
    "        self.tau = 10e-3  # 1000Hz 采样频率，可以理解为帧率的倒数\n",
    "\n",
    "        self.theta_threshold_radians: float = (\n",
    "            30 * 2 * math.pi / 360\n",
    "        )  # 30度，杆子最大倾角，超过就算失败\n",
    "        self.x_threshold: float = 5  # 小车最大移动距离，超过就算失败\n",
    "\n",
    "        self.np_random = np.random.RandomState(seed)  # 环境随机数种子\n",
    "        self.max_steps = max_steps  # 每个回合的最大步数\n",
    "        self.state: State = State()  # 环境状态\n",
    "        self.steps: int = 0  # 当前步数\n",
    "\n",
    "    def reset(self, seed: int) -> State:\n",
    "        \"\"\"重置环境，返回初始状态\"\"\"\n",
    "        if seed is not None:\n",
    "            self.np_random.seed(seed)\n",
    "\n",
    "        state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        self.state = State(\n",
    "            x=state[0], x_dot=state[1], theta=state[2], theta_dot=state[3]\n",
    "        )\n",
    "        self.steps = 0\n",
    "\n",
    "        return self.state.model_copy()\n",
    "\n",
    "    def step(self, action: Action) -> Step:\n",
    "        \"\"\"每一步骤的环境交互\"\"\"\n",
    "        x = self.state.x\n",
    "        x_dot = self.state.x_dot\n",
    "        theta = self.state.theta\n",
    "        theta_dot = self.state.theta_dot\n",
    "\n",
    "        force = self.force_mag if action is Action.RIGHT else -self.force_mag\n",
    "\n",
    "        cos_theta = math.cos(self.state.theta)\n",
    "        sin_theta = math.sin(self.state.theta)\n",
    "\n",
    "        temp = (\n",
    "            force + self.pole_mass_length * theta_dot**2 * sin_theta\n",
    "        ) / self.total_mass\n",
    "        theta_acc = (self.gravity * sin_theta - cos_theta * temp) / (\n",
    "            self.pole_length\n",
    "            * (4.0 / 3.0 - self.pole_mass * cos_theta**2 / self.total_mass)\n",
    "        )\n",
    "\n",
    "        x_acc = temp - self.pole_mass_length * theta_acc * cos_theta / self.total_mass\n",
    "\n",
    "        x = x + self.tau * x_dot\n",
    "        x_dot = x_dot + self.tau * x_acc\n",
    "        theta = theta + self.tau * theta_dot\n",
    "        theta_dot = theta_dot + self.tau * theta_acc\n",
    "\n",
    "        self.state = State(x=x, x_dot=x_dot, theta=theta, theta_dot=theta_dot)\n",
    "        self.steps += 1\n",
    "\n",
    "        terminated = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "        truncated = self.steps >= self.max_steps\n",
    "        done = terminated or truncated\n",
    "        if terminated:\n",
    "            reward = 0.0\n",
    "        else:\n",
    "            reward = 1.0\n",
    "\n",
    "        return Step(\n",
    "            state=self.state.model_copy(),\n",
    "            reward=reward,\n",
    "            done=done,\n",
    "            terminated=terminated,\n",
    "            truncated=truncated,\n",
    "            info={},\n",
    "        )\n",
    "\n",
    "\n",
    "# 用经典阈值近似归一化尺度\n",
    "X_MAX, X_DOT_MAX = 2.4, 3.0  # 3.0 是经验上限\n",
    "THETA_MAX, THETA_DOT_MAX = 12 * 2 * np.pi / 360, 3.5\n",
    "\n",
    "\n",
    "class LinearQNet(LinearQNet):\n",
    "    def __init__(self, cfg: CartPoleConfig, seed=42) -> None:\n",
    "        super().__init__(cfg, seed)\n",
    "        self.cfg = cfg\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        self.step = 0\n",
    "\n",
    "        # 先构造一个“占位状态”，用来推断 phi 维度\n",
    "        _dummy = State()\n",
    "        _phi = self._phi_vec(_dummy)  # <-- 统一使用 _phi_vec\n",
    "        self.phi_dim = _phi.shape[0]  # <-- 自动推断维度\n",
    "        self.W = self.rng.randn(cfg.n_actions, self.phi_dim) * 0.01\n",
    "\n",
    "    # === 新增：统一产出 numpy 向量的函数（你可以把缩放/特征工程都放这里）===\n",
    "    def _phi_vec(self, s: State) -> np.ndarray:\n",
    "        # 经典阈值做归一化（可按需调整）\n",
    "        X_MAX, X_DOT_MAX = 2.4, 3.0\n",
    "        THETA_MAX, THETA_DOT_MAX = 12 * 2 * np.pi / 360, 3.5\n",
    "\n",
    "        sx = np.clip(s.x / X_MAX, -1, 1)\n",
    "        sxd = np.clip(s.x_dot / X_DOT_MAX, -1, 1)\n",
    "        st = np.clip(s.theta / THETA_MAX, -1, 1)\n",
    "        std = np.clip(s.theta_dot / THETA_DOT_MAX, -1, 1)\n",
    "\n",
    "        # 与你采用的“8 维特征”保持一致（顺序固定！）\n",
    "        return np.array(\n",
    "            [\n",
    "                sx,\n",
    "                sxd,\n",
    "                np.sin(s.theta),\n",
    "                np.cos(s.theta),\n",
    "                std,\n",
    "                sx * sxd,\n",
    "                st * std,\n",
    "                self.cfg.bias,  # bias\n",
    "            ],\n",
    "            dtype=float,\n",
    "        )\n",
    "\n",
    "    # === 仍然保留你写的 Pydantic 特征（如果还用得上），但 q_values 不再依赖它的顺序 ===\n",
    "    def phi(self, s: State) -> Feature:\n",
    "        return Feature(\n",
    "            x=s.x,\n",
    "            x2=s.x**2,\n",
    "            x_dot=s.x_dot,\n",
    "            x_dot2=s.x_dot**2,\n",
    "            theta=s.theta,\n",
    "            theta2=s.theta**2,\n",
    "            theta_dot=s.theta_dot,\n",
    "            theta_dot2=s.theta_dot**2,\n",
    "            bias=self.cfg.bias,\n",
    "        )\n",
    "\n",
    "    def epsilon(self) -> float:\n",
    "        t = min(self.step, self.cfg.eps_steps)\n",
    "        return (\n",
    "            self.cfg.eps_start\n",
    "            + t * (self.cfg.eps_end - self.cfg.eps_start) / self.cfg.eps_steps\n",
    "        )\n",
    "\n",
    "    def q_values(self, s: State) -> np.ndarray:\n",
    "        phi_vec = self._phi_vec(s)  # <-- 统一使用同一份特征\n",
    "        return self.W @ phi_vec\n",
    "\n",
    "    def act(self, s: State) -> Action:\n",
    "        self.step += 1\n",
    "        if self.rng.rand() < self.epsilon():\n",
    "            return self.rng.choice([Action.LEFT, Action.RIGHT])\n",
    "        best_a = int(np.argmax(self.q_values(s)))\n",
    "        return Action.RIGHT if best_a == 1 else Action.LEFT\n",
    "\n",
    "    def update_td0(self, s: State, a: Action, r: float, s_: State, done: bool):\n",
    "        phi_vec = self._phi_vec(s)  # <-- 同一份特征\n",
    "        q_vals = self.q_values(s)\n",
    "        q_sa = q_vals[1] if a == Action.RIGHT else q_vals[0]\n",
    "\n",
    "        if done:\n",
    "            target = r\n",
    "        else:\n",
    "            target = r + self.cfg.gamma * np.max(self.q_values(s_))\n",
    "\n",
    "        td_err = target - q_sa\n",
    "        a_idx = 1 if a == Action.RIGHT else 0\n",
    "        self.W[a_idx] += self.cfg.alpha * td_err * phi_vec\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class Replay:\n",
    "    def __init__(self, cap=50000, rng=None):\n",
    "        self.buf = deque(maxlen=cap)\n",
    "        self.rng = np.random.RandomState(0) if rng is None else rng\n",
    "\n",
    "    def push(self, s, a, r, s_, done):\n",
    "        self.buf.append((s, a, r, s_, done))\n",
    "\n",
    "    def can_sample(self, bs):\n",
    "        return len(self.buf) >= bs\n",
    "\n",
    "    def sample(self, bs):\n",
    "        idx = self.rng.choice(len(self.buf), size=bs, replace=False)\n",
    "        batch = [self.buf[i] for i in idx]\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4a114a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def train(\n",
    "    cfg: CartPoleConfig = CartPoleConfig(), episodes: int = 300\n",
    ") -> tuple[LinearQNet, List[float]]:\n",
    "    env = CartPoleMini(seed=42, max_steps=5000)\n",
    "    agent = LinearQNet(cfg, seed=42)\n",
    "\n",
    "    # 初始化经验回放\n",
    "    replay = Replay(cap=10000)  # 设置合适的容量\n",
    "    batch_size = 32  # 批量大小\n",
    "\n",
    "    n_episodes = episodes\n",
    "    rewards: List[float] = []\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset(seed=episode)\n",
    "        total_reward = 0\n",
    "\n",
    "        while True:\n",
    "            action = agent.act(state)\n",
    "            step_result = env.step(action)\n",
    "            state_, reward, done = (\n",
    "                step_result.state,\n",
    "                step_result.reward,\n",
    "                step_result.done,\n",
    "            )\n",
    "\n",
    "            # 存储经验到回放 buffer\n",
    "            replay.push(state, action, reward, state_, done)\n",
    "\n",
    "            # 从回放 buffer 中采样学习\n",
    "            if replay.can_sample(batch_size):\n",
    "                batch = replay.sample(batch_size)\n",
    "                for s, a, r, s_, d in batch:\n",
    "                    agent.update_td0(s, a, r, s_, d)  # 使用采样的经验更新\n",
    "\n",
    "            state = state_\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            avg_reward = np.mean(rewards[-10:])\n",
    "            print(\n",
    "                f\"Episode {episode + 1}, Average Reward: {avg_reward:.2f}, Epsilon: {agent.epsilon():.3f}\"\n",
    "            )\n",
    "\n",
    "    return agent, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d1bbf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent: LinearQNet, episodes: int = 20, deterministic=True) -> List[float]:\n",
    "    rets: List[float] = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        env = CartPoleMini(seed=ep, max_steps=5000)\n",
    "        state = env.reset(seed=ep)\n",
    "        total_reward = 0\n",
    "\n",
    "        while True:\n",
    "            if deterministic:\n",
    "                q_vals = agent.q_values(state)\n",
    "                best_a = np.argmax(q_vals)\n",
    "                action = Action.RIGHT if best_a == 1 else Action.LEFT\n",
    "            else:\n",
    "                action = agent.act(state)\n",
    "\n",
    "            step_result = env.step(action)\n",
    "            state_, reward, done = (\n",
    "                step_result.state,\n",
    "                step_result.reward,\n",
    "                step_result.done,\n",
    "            )\n",
    "\n",
    "            state = state_\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rets.append(total_reward)\n",
    "\n",
    "    return rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bcc37da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练（无回放、无目标网络、在线 TD(0)、线性 Q）...\n",
      "Episode 10, Average Reward: 66.90, Epsilon: 0.677\n",
      "Episode 20, Average Reward: 68.60, Epsilon: 0.347\n",
      "Episode 30, Average Reward: 65.30, Epsilon: 0.050\n",
      "Episode 40, Average Reward: 65.80, Epsilon: 0.050\n",
      "Episode 50, Average Reward: 66.60, Epsilon: 0.050\n",
      "Episode 60, Average Reward: 69.50, Epsilon: 0.050\n",
      "Episode 70, Average Reward: 67.30, Epsilon: 0.050\n",
      "Episode 80, Average Reward: 62.70, Epsilon: 0.050\n",
      "Episode 90, Average Reward: 66.50, Epsilon: 0.050\n",
      "Episode 100, Average Reward: 63.90, Epsilon: 0.050\n",
      "Episode 110, Average Reward: 68.20, Epsilon: 0.050\n",
      "Episode 120, Average Reward: 65.60, Epsilon: 0.050\n",
      "Episode 130, Average Reward: 73.20, Epsilon: 0.050\n",
      "Episode 140, Average Reward: 67.20, Epsilon: 0.050\n",
      "Episode 150, Average Reward: 73.50, Epsilon: 0.050\n",
      "Episode 160, Average Reward: 71.30, Epsilon: 0.050\n",
      "Episode 170, Average Reward: 66.80, Epsilon: 0.050\n",
      "Episode 180, Average Reward: 66.90, Epsilon: 0.050\n",
      "Episode 190, Average Reward: 65.40, Epsilon: 0.050\n",
      "Episode 200, Average Reward: 70.10, Epsilon: 0.050\n",
      "Episode 210, Average Reward: 64.50, Epsilon: 0.050\n",
      "Episode 220, Average Reward: 67.70, Epsilon: 0.050\n",
      "Episode 230, Average Reward: 67.20, Epsilon: 0.050\n",
      "Episode 240, Average Reward: 70.30, Epsilon: 0.050\n",
      "Episode 250, Average Reward: 69.60, Epsilon: 0.050\n",
      "Episode 260, Average Reward: 72.30, Epsilon: 0.050\n",
      "Episode 270, Average Reward: 66.80, Epsilon: 0.050\n",
      "Episode 280, Average Reward: 72.60, Epsilon: 0.050\n",
      "Episode 290, Average Reward: 70.20, Epsilon: 0.050\n",
      "Episode 300, Average Reward: 65.40, Epsilon: 0.050\n",
      "Episode 310, Average Reward: 67.10, Epsilon: 0.050\n",
      "Episode 320, Average Reward: 67.90, Epsilon: 0.050\n",
      "Episode 330, Average Reward: 69.80, Epsilon: 0.050\n",
      "Episode 340, Average Reward: 70.10, Epsilon: 0.050\n",
      "Episode 350, Average Reward: 68.40, Epsilon: 0.050\n",
      "Episode 360, Average Reward: 65.30, Epsilon: 0.050\n",
      "Episode 370, Average Reward: 65.80, Epsilon: 0.050\n",
      "Episode 380, Average Reward: 68.10, Epsilon: 0.050\n",
      "Episode 390, Average Reward: 69.50, Epsilon: 0.050\n",
      "Episode 400, Average Reward: 70.70, Epsilon: 0.050\n",
      "Episode 410, Average Reward: 67.90, Epsilon: 0.050\n",
      "Episode 420, Average Reward: 74.30, Epsilon: 0.050\n",
      "Episode 430, Average Reward: 70.20, Epsilon: 0.050\n",
      "Episode 440, Average Reward: 65.50, Epsilon: 0.050\n",
      "Episode 450, Average Reward: 68.00, Epsilon: 0.050\n",
      "Episode 460, Average Reward: 64.80, Epsilon: 0.050\n",
      "Episode 470, Average Reward: 69.30, Epsilon: 0.050\n",
      "Episode 480, Average Reward: 68.20, Epsilon: 0.050\n",
      "Episode 490, Average Reward: 66.70, Epsilon: 0.050\n",
      "Episode 500, Average Reward: 68.00, Epsilon: 0.050\n",
      "评估...\n",
      "[64.0, 91.0, 66.0, 74.0, 55.0, 73.0, 62.0, 67.0, 58.0, 70.0, 62.0, 67.0, 75.0, 57.0, 61.0, 87.0, 69.0, 83.0, 60.0, 79.0]\n"
     ]
    }
   ],
   "source": [
    "env = CartPoleMini(seed=42, max_steps=5000)\n",
    "cfg = CartPoleConfig(alpha=5e-3, gamma=0.99, eps_steps=2000, eps_end=0.05)\n",
    "agent = LinearQNet(cfg, seed=0)\n",
    "\n",
    "print(\"开始训练（无回放、无目标网络、在线 TD(0)、线性 Q）...\")\n",
    "agent, rewards = train(cfg, 500)\n",
    "\n",
    "print(\"评估...\")\n",
    "results = evaluate(agent, 20, deterministic=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1051af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
