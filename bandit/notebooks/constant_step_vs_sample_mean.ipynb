{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e7e6650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "import uuid\n",
    "from typing import Tuple, List, Type\n",
    "from datetime import datetime\n",
    "import gc\n",
    "\n",
    "from bandit_lib.runner import batch_train\n",
    "from bandit_lib.agents import (\n",
    "    BaseRewardStates,\n",
    "    Metrics,\n",
    "    MetricsConfig,\n",
    "    GreedyConfig,\n",
    "    GreedyAgent,\n",
    "    AlgorithmConfig,\n",
    ")\n",
    "from bandit_lib.env import EnvConfig\n",
    "from bandit_lib.agents.base import Agent_T\n",
    "from bandit_lib.utils import (\n",
    "    save_process_data,\n",
    "    ProcessDataDump,\n",
    "    save_meta_data,\n",
    "    MetaDataDump,\n",
    "    plot_metrics_history,\n",
    "    get_metric_labels,\n",
    "    plot_comparison,\n",
    "    build_confidence_table_from_runs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4332bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "ROOT = Path.cwd().parent  # Root Path\n",
    "DATE = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "EXP_ID = uuid.uuid4().hex  # You can find logs and figures in the results folders, path: results/logs, results/figures\n",
    "METRIC_LABELS = get_metric_labels()\n",
    "\n",
    "# states\n",
    "# environment\n",
    "ENABLE_DYNAMIC = True\n",
    "RANDOM_WALK_ARM_NUM = 1\n",
    "RANDOM_WALK_INTERNAL = 1\n",
    "RANDOM_WALK_STD = 0.01\n",
    "ARM_NUM = 10\n",
    "\n",
    "# experiment\n",
    "REPEAT_TIMES = 500  # The number of repititions for each test run should be set to 500 to guarantee consistent and reliable outcomes\n",
    "STEP_NUM = 100_000\n",
    "WORKER_NUM = 10\n",
    "BASE_SEED = 42  # The base seed for multiple parallel replications under different variables, which increments by one for each run, hence called base seed\n",
    "\n",
    "# algorithm\n",
    "OPTIMISTIC_INITIALIZATION_ENABLED = True\n",
    "OPTIMISTIC_INITIALIZATION_VALUE = 1\n",
    "\n",
    "# configs\n",
    "ENV_CONFIG = EnvConfig(\n",
    "    enable_dynamic=ENABLE_DYNAMIC,\n",
    "    random_walk_internal=RANDOM_WALK_INTERNAL,\n",
    "    random_walk_arm_num=RANDOM_WALK_ARM_NUM,\n",
    "    random_walk_std=RANDOM_WALK_STD,\n",
    ")\n",
    "METRICS_CONFIG = MetricsConfig(metrics_history_size=500)\n",
    "GREEDY_CONFIG = GreedyConfig(\n",
    "    optimistic_initialization_enabled=OPTIMISTIC_INITIALIZATION_ENABLED,\n",
    "    optimistic_initialization_value=OPTIMISTIC_INITIALIZATION_VALUE,\n",
    ")\n",
    "\n",
    "# results\n",
    "RESULTS_LIST: List[Tuple[ProcessDataDump, List[GreedyAgent]]] = []\n",
    "\n",
    "# experiment variables\n",
    "LEARN_RATE = [\n",
    "    0,\n",
    "    0.001,\n",
    "    0.005,\n",
    "    0.01,\n",
    "    0.05,\n",
    "    0.1,\n",
    "    0.3,\n",
    "    0.5,\n",
    "    0.7,\n",
    "    0.9,\n",
    "    0.99,\n",
    "    0.999,\n",
    "    0.9999,\n",
    "    0.99999,\n",
    "]\n",
    "\n",
    "# dynamic environment, remove convergence_rate from metric labels\n",
    "METRIC_LABELS.remove(\"convergence_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9565fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_id(name: str) -> str:\n",
    "    return f\"{name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "\n",
    "def run_agent(\n",
    "    run_id: str,\n",
    "    agent_type: Type[Agent_T],\n",
    "    name: str,\n",
    "    arm_num: int,\n",
    "    env_config: EnvConfig,\n",
    "    algorithm_config: AlgorithmConfig,\n",
    "    repeat_times: int,\n",
    "    step_num: int,\n",
    "    base_seed: int,\n",
    "    worker_num: int,\n",
    "    metrics_config: MetricsConfig,\n",
    "    metrics_to_plot: List[str],\n",
    ") -> Tuple[ProcessDataDump, List[Agent_T]]:\n",
    "    _results: Tuple[List[Agent_T], BaseRewardStates, List[Metrics]] = batch_train(\n",
    "        run_id=run_id,\n",
    "        agent_type=agent_type,\n",
    "        name=name,\n",
    "        arm_num=arm_num,\n",
    "        env_config=env_config,\n",
    "        algorithm_config=algorithm_config,\n",
    "        repeat_times=repeat_times,\n",
    "        step_num=step_num,\n",
    "        base_seed=base_seed,\n",
    "        worker_num=worker_num,\n",
    "        metrics_config=metrics_config,\n",
    "    )\n",
    "\n",
    "    save_meta_data(\n",
    "        meta_data=MetaDataDump(\n",
    "            experiment_date=DATE,\n",
    "            experiment_id=EXP_ID,\n",
    "            agent_runs_num=repeat_times,\n",
    "            total_steps=step_num,\n",
    "            arm_num=arm_num,\n",
    "            agent_seed=base_seed,\n",
    "            algorithm=algorithm_config,\n",
    "            metrics_config=metrics_config,\n",
    "            env_config=env_config,\n",
    "        ),\n",
    "        path=ROOT / \"results\" / \"logs\" / DATE / f\"meta_{run_id}.json\",\n",
    "    )\n",
    "    metrics_history: List[List[Metrics]] = []\n",
    "    for agent in _results[0]:\n",
    "        metrics_history.append(\n",
    "            [metric.model_copy(deep=True) for metric in agent.metrics]\n",
    "        )\n",
    "    process_data = ProcessDataDump(\n",
    "        run_id=run_id,\n",
    "        create_at=datetime.now(),\n",
    "        rewards=_results[1],\n",
    "        metrics=_results[2][-1],\n",
    "        metrics_history=metrics_history,\n",
    "        metrics_history_avg=_results[2],\n",
    "    )\n",
    "    save_process_data(\n",
    "        process_data=process_data,\n",
    "        path=ROOT / \"results\" / \"logs\" / DATE / f\"process_{run_id}.json\",\n",
    "    )\n",
    "    plot_metrics_history(\n",
    "        metrics_history=_results[2],\n",
    "        agent_name=name,\n",
    "        file_name=ROOT / \"results\" / \"figures\" / DATE / f\"{run_id}.html\",\n",
    "        agents=_results[0],\n",
    "        x_log=True,\n",
    "        metrics_to_plot=metrics_to_plot,\n",
    "        enable_statistical_credibility=True,\n",
    "    )\n",
    "    plot_metrics_history(\n",
    "        metrics_history=_results[2],\n",
    "        agent_name=name,\n",
    "        file_name=ROOT / \"results\" / \"figures\" / DATE / f\"{run_id}.jpeg\",\n",
    "        agents=_results[0],\n",
    "        x_log=True,\n",
    "        width=1500,\n",
    "        height=1000,\n",
    "        scale=2,\n",
    "        metrics_to_plot=metrics_to_plot,\n",
    "        enable_statistical_credibility=True,\n",
    "    )\n",
    "    results = (process_data, _results[0])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d582deba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GreedyAgent BaseLine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training agents:  18%|█▊        | 91/500 [00:16<00:53,  7.66it/s]"
     ]
    }
   ],
   "source": [
    "for learn_rate in LEARN_RATE:\n",
    "    GREEDY_CONFIG.learning_rate = learn_rate\n",
    "    print(f\"Running {GreedyAgent.__name__} with learn_rate={learn_rate}\" if learn_rate != 0 else f\"Running {GreedyAgent.__name__} BaseLine\")\n",
    "    _results = run_agent(\n",
    "        run_id=get_run_id(name=f\"learn_rate_{learn_rate}_{GreedyAgent.__name__}\" if learn_rate != 0 else f\"baseLine_{GreedyAgent.__name__}\"),\n",
    "        agent_type=GreedyAgent,\n",
    "        name=f\"{GreedyAgent.__name__} Learn rate={learn_rate}\" if learn_rate != 0 else f\"{GreedyAgent.__name__} BaseLine\",\n",
    "        arm_num=ARM_NUM,\n",
    "        env_config=ENV_CONFIG,\n",
    "        algorithm_config=GREEDY_CONFIG,\n",
    "        repeat_times=REPEAT_TIMES,\n",
    "        step_num=STEP_NUM,\n",
    "        base_seed=BASE_SEED,\n",
    "        worker_num=WORKER_NUM,\n",
    "        metrics_config=METRICS_CONFIG,\n",
    "        metrics_to_plot=METRIC_LABELS,\n",
    "    )\n",
    "    RESULTS_LIST.append(_results)\n",
    "    print(f\"Finished {GreedyAgent.__name__} with learn_rate={learn_rate}\" if learn_rate != 0 else f\"Finished {GreedyAgent.__name__} BaseLine\")\n",
    "    print(gc.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c50522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot comparison\n",
    "fig_comparison = plot_comparison(\n",
    "    runs_data=RESULTS_LIST,\n",
    "    file_name=ROOT / \"results\" / \"figures\" / DATE / f\"{EXP_ID}_x_log.html\",\n",
    "    metrics_to_plot=METRIC_LABELS,\n",
    "    show_intersections=False,\n",
    "    x_log=True,\n",
    "    enable_statistical_credibility=True,\n",
    ")\n",
    "plot_comparison(\n",
    "    runs_data=RESULTS_LIST,\n",
    "    file_name=ROOT / \"results\" / \"figures\" / DATE / f\"{EXP_ID}_x_log.png\",\n",
    "    metrics_to_plot=METRIC_LABELS,\n",
    "    show_intersections=False,\n",
    "    x_log=True,\n",
    "    enable_statistical_credibility=True,\n",
    ")\n",
    "fig_comparison.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaee889",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_confidence_table_from_runs(\n",
    "    runs_data=RESULTS_LIST,\n",
    "    metrics=METRIC_LABELS,\n",
    "    confidence=0.95,\n",
    "    digits=4,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
