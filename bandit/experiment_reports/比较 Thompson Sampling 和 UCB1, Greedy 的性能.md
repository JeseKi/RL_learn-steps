# 实验条件

* 环境：多臂老虎机
* 臂数 ($K$): 10
* 步数 ($T$): $10^5$
* 每个臂的的奖励分布：$X\sim\text{Bernoulli}(\theta_i)\\ \text{其中，}\theta_i=\frac{i}{K+1}\ ,(i\in\{1,2,3...,K\})$
* 算法：$Greedy$

  * 表达式：$a_{t+1} = \arg\max_{a\in \{1,2,3,...,A\}}\hat{Q}_t(a)\\ \text{其中，}\hat{Q}_t(a)= \begin{cases} Q_0 &\text{如果动作 a 还没有被选择过}\\ \frac{1}{N_t(a)}\sum_{i:A_i=a}^{t}R_i & \text{如果动作 a 已经被选择过} \end{cases}$
  * $Q_0 = 1$
  * 参数：

    * $\hat{Q}_t(a)\text{: 对动作 a 在 t 时刻的回报真实值估量}$
    * $N_t(a)\text{: 到时刻 t 为止，动作 a 的执行次数}$
    * $A\text{: 动作空间}$
    * $R_i\text{: i 时刻的奖励}$
    * $Q_0\text{: 乐观初始化值}$
* 算法：$UCB1$

  * 表达式：$a_{t+1}=\arg\max_{a\in{\{1, 2, 3, ..., A\}}}\left[\ \hat{Q}_t(a)+\sqrt{\frac{2\ln{t}}{N_t(a)}}\ \right]\\ \text{其中，}\hat{Q}_t(a)=\frac{1}{N_t(a)}\sum_{i:A_i=a}^{t}R_i $
  * 参数：

    * $\hat{Q}_t(a)\text{: 对动作 a 在 t 时刻的回报真实值估量}$
    * $N_t(a)\text{: 到时刻 t 为止，动作 a 的执行次数}$
    * $A\text{动作空间}$
    * $R_i\text{: i 时刻的奖励}$
* 算法：$Thompson\ Sampling$

  * 表达式：$a_{t+1}=\arg\max_{a\in\{1,2,3,...,A\}}\tilde{Q}_t(a)\\ \text{其中，} \tilde{Q}_t(a)\sim\text{Beta}\left(S_t(a)+1, F_t(a)+1\right)$

    参数：

    * $\tilde{Q}_t(a)\text{: 对于动作 a 从 Beta 后验分布中采样得到的动作价值}$
    * $S_t(a)\text{: 到时刻 t 为止动作 a 获得成功的次数}$
    * $F_t(a)\text{: 到时刻 t 为止动作 a 获得失败的次数}$
* 重复运行次数($R$)：500

  * 说明：同一次重复运行中，每个 Agent 的种子均不同，不同重复运行的 Agent 种子均相同。所有重复运行的环境种子均相同。
* 实验目的：比较不同步数下 UCB1 和贪心算法的性能

  * 对比不同的$K$下：

    * 累积奖励的曲线如何随时间的变化
    * 累积的后悔值曲线
    * 后悔率曲线
    * 最佳臂命中率曲线
    * 收敛率
    * 收敛速度（多少步达到收敛）
* 收敛：最佳臂命中率达到 90% 及以上时，可以认为是收敛

# 实验记录

## 指标对比

| **算法名称** |        **遗憾值** |       **遗憾率** |         **累积奖励** |       **最优动作率** |         **收敛步数** |   **收敛率**   |
| :-: | --------: | -------: | ---------: | -------: | ---------: | :-----: |
| $Greedy$ | 3660.13 | 0.0403 | 87248.96 | 0.7178 |   245.88 | 0.718 |
| $UCB1$ |  608.33 | 0.0067 | 90300.76 | 0.9662 | 24310.90 | 1.00 |
| $Thompson\ Sampling$ |   64.24 | 0.0007 | 90844.85 | 0.9971 |  1443.18 | 1.00 |

## 过程数据图表

| X 轴增长方式 | $Greedy$                                                                                                                           | $UCB1$                                                                                                                 | $Thompson\ Sampling$                                                                                                                             |
| -------------- | ---------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------ |
| 线性增长     | ![](https://tuchuang-1317479375.cos.ap-beijing.myqcloud.com/greedy_average_1757904542.498628_T=100000_K=10_Q_0=1.png)       | ![](https://tuchuang-1317479375.cos.ap-beijing.myqcloud.com/ucb1_1757904650.147792_T=100000_K=10_Q_0=1.png)       | ![](https://tuchuang-1317479375.cos.ap-beijing.myqcloud.com/thompson_sampling_1757905079.73072_T=100000_K=10_Q_0=1.png)       |
| 指数增长     | ![](https://tuchuang-1317479375.cos.ap-beijing.myqcloud.com/greedy_average_1757904542.498628_T=100000_K=10_Q_0=1_x_log.png) | ![](https://tuchuang-1317479375.cos.ap-beijing.myqcloud.com/ucb1_1757904650.147792_T=100000_K=10_Q_0=1_x_log.png) | ![](https://tuchuang-1317479375.cos.ap-beijing.myqcloud.com/thompson_sampling_1757905079.73072_T=100000_K=10_Q_0=1_x_log.png) |

## 过程数据图表 (Greedy vs UCB1 vs TS)

| X-线性增长                                                                      | X-指数增长                                                                           |
| --------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- |
| ![](https://tuchuang-1317479375.cos.ap-beijing.myqcloud.com/metrics_compare.png) | ![](https://tuchuang-1317479375.cos.ap-beijing.myqcloud.com/metrics_compare_logx.png) |

# 实验现象

* 早期阶段($t<500$)：从过程数据图表(Greedy vs UCB1 vs TS)的 X-指数增长的这张图中可以清晰的看到，在最初的几十步中，Greedy 凭借其乐观初始化($Q_0=1$)的早期优势，迅速锁定的目前看起来最优的臂，因此其命中率早期（约$t\in{\left[20, 4604\right]}$）领先于正在进行广泛探索的 UCB1 算法和 TS 算法。
* 中期探索与超越：随着步数的增加，UCB1 和 TS 算法通过持续的探索，逐渐获得了对所有臂更准确的价值估计。

  * 在$t\approx224$步以后，TS 算法的命中率超越了 Greedy 算法。
  * 而 UCB1 的探索则更为保守和漫长，直到$t\approx4604$步后，其命中才完成对 Greedy 算法的超越。
* 后期收敛状态：在试验后期（$t\rightarrow10^5$）

  * Greedy 的命中率收敛于约 **71.8%** ，表明其已陷入局部最优，无法稳定选择真正奖励最高的臂。
  * UCB1 和 TS 算法都表现出了优秀的收敛性，命中率均趋近于 **100%** 。TS 算法在 $t\approx1443$ 步时命中率就达到了 90% 的收敛标准，而 UCB1 则需要约 $t\approx24311$，**显示出 TS 在收敛速度上远优于 UCB1。**

为了验证最终收敛率的差异是否显著，我们进行了 Z 检验，结果显示：

* TS vs UCB1: $z=3.64, p<0.01$
* UCB1 vs Greedy: $z=10.77, p<0.001$

这在统计学上证明了三种算法在最终最佳臂选择能力上存在显著差异，与我们的观感相符：

$$
\boxed{r_{TS}\triangleright{r_{UCB1}}\triangleright{r_{Greedy}}}
$$

3. **累积奖励分析**

累积奖励直观地反映了算法在整个过程中的收益。

* 曲线整体上，三种算法的累积奖励都随步数增加而近似线性增长。但是，**增长的斜率代表了算法在稳定状态下选择臂的平均奖励**。
* 在$t>10027$步之后，UCB1 和 TS 算法的累积奖励曲线斜率明显高于 Greedy 算法，并最终与之拉开显著差异。
* TS 和 UCB1 在实验后期累积奖励值非常接近，但 TS 凭借更快的收敛速度，在整个生命周期内获得了略高的总奖励（90844.85 vs 90300.76）

4. **累积后悔值和后悔率分析**

后悔值是衡量探索与利用代价的关键指标。

* 累积后悔值：

  * Greedy 算法的累积后悔值呈现出线性增长的趋势，这正是陷入局部最优的直接体现——在每个时间步，几乎都是在犯同样的“错误”，从而累积了线性增长的遗憾。
  * 相比之下，UCB1 和 TS 算法则呈现出对数增长的趋势。曲线在早期斜率较大（探索代价），后期则趋于平缓，表明算法收敛后产生的后悔值极低。
* 后悔率：

  * 所有算法的后悔率在一开始都迅速下降。
  * Greedy 的后悔率在$t\approx520$后趋于一个非零的常数（约0.04），这意味着它平均每一步都会损失固定的奖励。
  * UCB1 和 TS 的后悔率则持续下降并无限趋近于 0，表明它们最终能做到几乎“不再后悔”。其中，TS 的后悔率下降速度显著快于 UCB1。

# 实验结论

本次实验系统地比较了 Greedy, UCB1, 和 Thompson Sampling (TS) 算法的在静态多臂老虎机问题上的性能。实验结论如下：

1. **核心结论：Thompson Sampling 的综合性能最优，它在“探索与利用”的权衡中展现了最高的效率。** Greedy 算法因为无法进行有效的探索而陷入局部最优，而 UCB1 算法虽然能保证收敛，但其探索代价过高。
2. **算法策略的权衡分析：**

    * **Greedy (纯利用策略)** ：该算法验证了纯利用策略的内在缺陷。尽管其乐观初始化在实验极早期（约前 500 步）表现出优势，但一旦因随机性选择了次优臂，便缺乏纠错机制，导致其长期性能最差，累积后悔值呈线性增长。
    * **UCB1 (确定性探索策略)：** 该算法代表了一种稳健但保守的探索方式。它通过置信上界项强制对选择次数最少的臂进行探索，从而保证了**渐进最优性，** 最终能够收敛。然而，这种系统性的探索的效率并不高，导致在**中后期**（**直到约10027步前**）性能均落后于 Greedy ,是一种典型的“牺牲短期收益以换取长期保障”的策略。
    * **Thompson Sampling (概率性探索策略)** ：该算法通过对臂真实价值的后验分布进行采样，实现了一种更智能、更高效的探索。它根据不确定性的大小来自然地调整探索力度，因此能比 UCB1 **更快地（约 1443 步达到 90%最佳臂命中率）** 识别出最优臂，同时有效避免了 Greedy 的局部最优陷阱。
3. **总结和启示：** 实验结果明确表明，在静态 Bandit 问题中，**算法对“不确定性”的建模和处理方式是决定其性能的关键。** Thompson Sampling 所代表的贝叶斯方法，能够高效地量化和利用不确定性信息指导决策，从而在整体性能上超越了纯贪心策略和基于确定性区间的策略。