{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "854eb06c",
   "metadata": {},
   "source": [
    "# 导入必要库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1fb3304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import gc\n",
    "import time\n",
    "\n",
    "from core import RLEnv\n",
    "from core import BaseAgent\n",
    "from core.schemas import PiecewizeMethod  # noqa\n",
    "from greedy import EpsilonDecreasingConfig, GreedyAgent, GreedyAlgorithm, GreedyType\n",
    "from ucb1 import UCBAgent, UCB1Algorithm\n",
    "from thompson_sampling import TSAgent, TSAlgorithm\n",
    "\n",
    "from train import batch_train\n",
    "from utils import plot_metrics_history, save_experiment_data, ProcessDataLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c2d22f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS: int = 100_000\n",
    "GRID_SIZE: int = 500\n",
    "\n",
    "SEED: int = 42\n",
    "MACHINE_COUNT: int = 10\n",
    "RUN_COUNT: int = 5\n",
    "CONVERGENCE_THRESHOLD: float = 0.9\n",
    "CONVERGENCE_MIN_STEPS: int = 100\n",
    "OPTIMISTIC_TIMES: int = 1\n",
    "ENABLE_OPTIMISTIC: bool = True\n",
    "EXPERIMENT_DATA_DIR: Path = Path.cwd() / \"experiment_data\"\n",
    "\n",
    "ENV: RLEnv = RLEnv(\n",
    "    machine_count=MACHINE_COUNT,\n",
    "    random_walk_internal=1,\n",
    "    random_walk_machine_num=1,\n",
    "    # piecewise_internal=1,\n",
    "    # piecewize_method=PiecewizeMethod.UPSIDE_DOWN,\n",
    "    seed=SEED,\n",
    ")\n",
    "EPSILON_CONFIG: EpsilonDecreasingConfig = EpsilonDecreasingConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40e091e",
   "metadata": {},
   "source": [
    "# 工厂函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f02e2844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_id(agent_name: str) -> str:\n",
    "    return agent_name + \"_\" + str(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77de71c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_greedy_agent(\n",
    "    env: RLEnv,\n",
    "    epsilon_config: EpsilonDecreasingConfig,\n",
    "    optimistic_init: bool,\n",
    "    optimistic_times: int,\n",
    "    convergence_threshold: float,\n",
    "    convergence_min_steps: int,\n",
    "    seed: int,\n",
    ") -> BaseAgent:\n",
    "    return GreedyAgent(\n",
    "        name=GreedyType.GREEDY,\n",
    "        env=env,\n",
    "        algorithm=GreedyAlgorithm(\n",
    "            greedy_type=GreedyType.GREEDY,\n",
    "            optimistic_init=optimistic_init,\n",
    "            optimistic_times=optimistic_times,\n",
    "        ),\n",
    "        epsilon_config=epsilon_config,\n",
    "        convergence_threshold=convergence_threshold,\n",
    "        convergence_min_steps=convergence_min_steps,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_epsilon_agent(\n",
    "    env: RLEnv,\n",
    "    epsilon_config: EpsilonDecreasingConfig,\n",
    "    optimistic_init: bool,\n",
    "    optimistic_times: int,\n",
    "    convergence_threshold: float,\n",
    "    convergence_min_steps: int,\n",
    "    seed: int,\n",
    ") -> BaseAgent:\n",
    "    return GreedyAgent(\n",
    "        name=GreedyType.EPSILON,\n",
    "        env=env,\n",
    "        algorithm=GreedyAlgorithm(\n",
    "            greedy_type=GreedyType.EPSILON,\n",
    "            optimistic_init=optimistic_init,\n",
    "            optimistic_times=optimistic_times,\n",
    "        ),\n",
    "        epsilon_config=epsilon_config,\n",
    "        convergence_threshold=convergence_threshold,\n",
    "        convergence_min_steps=convergence_min_steps,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_decreasing_agent(\n",
    "    env: RLEnv,\n",
    "    epsilon_config: EpsilonDecreasingConfig,\n",
    "    optimistic_init: bool,\n",
    "    optimistic_times: int,\n",
    "    convergence_threshold: float,\n",
    "    convergence_min_steps: int,\n",
    "    seed: int,\n",
    ") -> BaseAgent:\n",
    "    return GreedyAgent(\n",
    "        name=GreedyType.EPSILON_DECREASING,\n",
    "        env=env,\n",
    "        algorithm=GreedyAlgorithm(\n",
    "            greedy_type=GreedyType.EPSILON_DECREASING,\n",
    "            optimistic_init=optimistic_init,\n",
    "            optimistic_times=optimistic_times,\n",
    "        ),\n",
    "        epsilon_config=epsilon_config,\n",
    "        convergence_threshold=convergence_threshold,\n",
    "        convergence_min_steps=convergence_min_steps,\n",
    "        seed=seed,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d41b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ucb1_agent(\n",
    "    env: RLEnv,\n",
    "    convergence_threshold: float,\n",
    "    convergence_min_steps: int,\n",
    "    seed: int,\n",
    ") -> BaseAgent:\n",
    "    return UCBAgent(\n",
    "        name=\"UCB1\",\n",
    "        env=env,\n",
    "        algorithm=UCB1Algorithm(),\n",
    "        convergence_threshold=convergence_threshold,\n",
    "        convergence_min_steps=convergence_min_steps,\n",
    "        seed=seed,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "079a4c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ts_agent(\n",
    "    env: RLEnv,\n",
    "    convergence_threshold: float,\n",
    "    convergence_min_steps: int,\n",
    "    seed: int,\n",
    ") -> BaseAgent:\n",
    "    return TSAgent(\n",
    "        name=\"Thompson Sampling\",\n",
    "        env=env,\n",
    "        algorithm=TSAlgorithm(),\n",
    "        convergence_threshold=convergence_threshold,\n",
    "        convergence_min_steps=convergence_min_steps,\n",
    "        seed=seed,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a0e0bb",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6993f9fc",
   "metadata": {},
   "source": [
    "## 普通贪婪算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f976af58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "达到收敛时的步数: 390\n",
      "达到收敛时的步数: 280\n",
      "达到收敛时的步数: 520\n",
      "达到收敛时的步数: 484\n",
      "达到收敛时的步数: 952\n",
      "avg_regret=24927.064841829404 avg_regret_rate=0.25573600015019016 avg_total_reward=72544.8 avg_optimal_rate=4.3995600439956004e-05 avg_convergence_steps=525.2 avg_convergence_rate=1.0\n",
      "values=[0.8, 2.4, 3746.4, 12537.2, 1.2, 28973.4, 0.4, 7971.4, 10395.4, 8916.2] counts=[1.8, 4.4, 5396.8, 16279.4, 2.6, 37366.6, 1.4, 10187.8, 16626.4, 14132.8]\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/greedy_1758166898.7507033_T=100000_K=10_Q_0=1.png\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/greedy_1758166898.7507033_T=100000_K=10_Q_0=1_x_log.png\n",
      "✅ 实验结果数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/greedy_1758166898.7507033_T=100000_K=10_Q_0=1.json\n",
      "✅ 过程数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/greedy_1758166898.7507033_T=100000_K=10_Q_0=1process.json\n",
      "974.6211862996641\n",
      "0.2418866870438949\n",
      "0.9746211862996641\n",
      "0.14394806390500214\n",
      "0.39942118229722756\n",
      "0.31320561292697163\n",
      "0.8655653081743138\n",
      "0.0529193811340304\n",
      "0.5988934339105028\n",
      "0.5339077818951432\n",
      "0.5189995776669264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26008"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id = get_run_id(GreedyType.GREEDY)\n",
    "file_name: Path = (\n",
    "    EXPERIMENT_DATA_DIR\n",
    "    / f\"{run_id}_T={STEPS}_K={MACHINE_COUNT}_Q_0={OPTIMISTIC_TIMES}.png\"\n",
    ")\n",
    "process_logger = ProcessDataLogger(\n",
    "    run_id=run_id,\n",
    "    total_steps=STEPS,\n",
    "    grid_size=GRID_SIZE,\n",
    ")\n",
    "\n",
    "agents, reward, metrics = batch_train(\n",
    "    count=RUN_COUNT,\n",
    "    agent_factory=create_greedy_agent,\n",
    "    env=ENV,\n",
    "    epsilon_config=EPSILON_CONFIG,\n",
    "    steps=STEPS,\n",
    "    seed=SEED,\n",
    "    optimistic_init=ENABLE_OPTIMISTIC,\n",
    "    optimistic_times=OPTIMISTIC_TIMES,\n",
    "    convergence_threshold=CONVERGENCE_THRESHOLD,\n",
    "    convergence_min_steps=CONVERGENCE_MIN_STEPS,\n",
    "    process_logger=process_logger,\n",
    ")\n",
    "print(metrics)\n",
    "print(reward)\n",
    "\n",
    "plot_metrics_history(agents, \"贪婪算法\", file_name, x_log=False)\n",
    "plot_metrics_history(agents, \"贪婪算法\", file_name, x_log=True)\n",
    "save_experiment_data(reward, metrics, file_name)\n",
    "process_logger.save(file_name.with_stem(file_name.stem + \"process\"), total_steps=STEPS)\n",
    "dump = process_logger.export(total_steps=STEPS)\n",
    "keys = list(dump.points[0].data.keys())\n",
    "\n",
    "print(ENV.best_reward(1000))\n",
    "for m in ENV.machines:\n",
    "    print(m.reward_probability)\n",
    "\n",
    "del agents, reward, metrics, process_logger, dump\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d0fa0",
   "metadata": {},
   "source": [
    "## UCB1算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "854e802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "达到收敛时的步数: 94622\n",
      "达到收敛时的步数: 300\n",
      "达到收敛时的步数: 9019\n",
      "达到收敛时的步数: 1594\n",
      "达到收敛时的步数: 2352\n",
      "avg_regret=45172.51529205487 avg_regret_rate=0.45925736269624184 avg_total_reward=53187.4 avg_optimal_rate=0.0 avg_convergence_steps=21577.4 avg_convergence_rate=1.0\n",
      "values=[53187.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] counts=[100000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/ucb1_1758166903.0016177_T=100000_K=10_Q_0=1.png\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/ucb1_1758166903.0016177_T=100000_K=10_Q_0=1_x_log.png\n",
      "✅ 实验结果数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/ucb1_1758166903.0016177_T=100000_K=10_Q_0=1.json\n",
      "✅ 过程数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/ucb1_1758166903.0016177_T=100000_K=10_Q_0=1process.json\n",
      "983.5991529205487\n",
      "0.41310498761286013\n",
      "0.27572572036770493\n",
      "0.6310892395563359\n",
      "0.7409620743653574\n",
      "0.4369300243356131\n",
      "0.8399288201770279\n",
      "0.31214177277867644\n",
      "0.11812537179593412\n",
      "0.9835991529205487\n",
      "0.8074088852828263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23782"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id = get_run_id(\"ucb1\")\n",
    "file_name: Path = (\n",
    "    EXPERIMENT_DATA_DIR\n",
    "    / f\"{run_id}_T={STEPS}_K={MACHINE_COUNT}_Q_0={OPTIMISTIC_TIMES}.png\"\n",
    ")\n",
    "process_logger = ProcessDataLogger(\n",
    "    run_id=run_id,\n",
    "    total_steps=STEPS,\n",
    "    grid_size=GRID_SIZE,\n",
    ")\n",
    "\n",
    "agents, reward, metrics = batch_train(\n",
    "    count=RUN_COUNT,\n",
    "    agent_factory=create_ucb1_agent,\n",
    "    env=ENV,\n",
    "    steps=STEPS,\n",
    "    seed=SEED,\n",
    "    convergence_threshold=CONVERGENCE_THRESHOLD,\n",
    "    convergence_min_steps=CONVERGENCE_MIN_STEPS,\n",
    "    process_logger=process_logger,\n",
    ")\n",
    "print(metrics)\n",
    "print(reward)\n",
    "\n",
    "plot_metrics_history(agents, \"UCB1 算法\", file_name, x_log=False)\n",
    "plot_metrics_history(agents, \"UCB1 算法\", file_name, x_log=True)\n",
    "save_experiment_data(reward, metrics, file_name)\n",
    "process_logger.save(file_name.with_stem(file_name.stem + \"process\"), total_steps=STEPS)\n",
    "dump = process_logger.export(total_steps=STEPS)\n",
    "keys = list(dump.points[0].data.keys())\n",
    "\n",
    "print(ENV.best_reward(1000))\n",
    "for m in ENV.machines:\n",
    "    print(m.reward_probability)\n",
    "\n",
    "del agents, reward, metrics, process_logger, dump\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9ffd59",
   "metadata": {},
   "source": [
    "# Thompson Sampling 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d294c600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "达到收敛时的步数: 6200\n",
      "达到收敛时的步数: 810\n",
      "avg_regret=2758.2555826458674 avg_regret_rate=0.03404183984911322 avg_total_reward=78267.2 avg_optimal_rate=0.039290000000000005 avg_convergence_steps=1402.0 avg_convergence_rate=0.4\n",
      "values=[17.8, 6909.6, 3092.8, 3869.6, 17491.6, 11548.0, 6013.0, 11290.8, 16827.6, 1206.4] counts=[33.0, 8496.6, 3929.0, 4908.2, 21346.2, 15154.6, 7595.6, 14809.8, 22178.0, 1549.0]\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/thompson_sampling_1758166906.808646_T=100000_K=10_Q_0=1.png\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/thompson_sampling_1758166906.808646_T=100000_K=10_Q_0=1_x_log.png\n",
      "✅ 实验结果数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/thompson_sampling_1758166906.808646_T=100000_K=10_Q_0=1.json\n",
      "✅ 过程数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/thompson_sampling_1758166906.808646_T=100000_K=10_Q_0=1process.json\n",
      "810.2545558264586\n",
      "0.4906028781252383\n",
      "0.19319058847102624\n",
      "0.8102545558264587\n",
      "0.4196510727439839\n",
      "0.327776544422129\n",
      "0.15961722059837177\n",
      "0.7123215153379453\n",
      "0.6283977561398881\n",
      "0.25251727492553033\n",
      "0.7847267258989756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22908"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id = get_run_id(\"thompson_sampling\")\n",
    "file_name: Path = (\n",
    "    EXPERIMENT_DATA_DIR\n",
    "    / f\"{run_id}_T={STEPS}_K={MACHINE_COUNT}_Q_0={OPTIMISTIC_TIMES}.png\"\n",
    ")\n",
    "process_logger = ProcessDataLogger(\n",
    "    run_id=run_id,\n",
    "    total_steps=STEPS,\n",
    "    grid_size=GRID_SIZE,\n",
    ")\n",
    "\n",
    "agents, reward, metrics = batch_train(\n",
    "    count=RUN_COUNT,\n",
    "    agent_factory=create_ts_agent,\n",
    "    env=ENV,\n",
    "    steps=STEPS,\n",
    "    seed=SEED,\n",
    "    convergence_threshold=CONVERGENCE_THRESHOLD,\n",
    "    convergence_min_steps=CONVERGENCE_MIN_STEPS,\n",
    "    process_logger=process_logger,\n",
    ")\n",
    "print(metrics)\n",
    "print(reward)\n",
    "\n",
    "plot_metrics_history(agents, \"TS 算法\", file_name, x_log=False)\n",
    "plot_metrics_history(agents, \"TS 算法\", file_name, x_log=True)\n",
    "save_experiment_data(reward, metrics, file_name)\n",
    "process_logger.save(file_name.with_stem(file_name.stem + \"process\"), total_steps=STEPS)\n",
    "dump = process_logger.export(total_steps=STEPS)\n",
    "keys = list(dump.points[0].data.keys())\n",
    "\n",
    "print(ENV.best_reward(1000))\n",
    "for m in ENV.machines:\n",
    "    print(m.reward_probability)\n",
    "\n",
    "del agents, reward, metrics, process_logger, dump\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
