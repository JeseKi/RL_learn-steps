{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "854eb06c",
   "metadata": {},
   "source": [
    "# 导入必要库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1fb3304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import os\n",
    "\n",
    "from core import RLEnv\n",
    "from core import BaseAgent\n",
    "from core.schemas import PiecewizeMethod  # noqa\n",
    "from greedy import EpsilonDecreasingConfig, GreedyAgent, GreedyAlgorithm, GreedyType\n",
    "from ucb1 import UCBAgent, UCB1Algorithm\n",
    "from thompson_sampling import TSAgent, TSAlgorithm\n",
    "from shemas import DynamicMethod\n",
    "\n",
    "from train import batch_train\n",
    "from utils import (\n",
    "    plot_metrics_history,\n",
    "    save_experiment_data,\n",
    "    ProcessDataLogger,\n",
    "    clear_var,\n",
    "    ExperimentMeta,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c2d22f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS: int = 100_000\n",
    "GRID_SIZE: int = 1000\n",
    "NUM_WORKERS: int = 10\n",
    "\n",
    "SEED: int = 42\n",
    "MACHINE_COUNT: int = 10\n",
    "RUN_COUNT: int = 500\n",
    "CONVERGENCE_THRESHOLD: float = 0.9\n",
    "CONVERGENCE_MIN_STEPS: int = 100\n",
    "OPTIMISTIC_TIMES: int = 1\n",
    "ENABLE_OPTIMISTIC: bool = True\n",
    "EXPERIMENT_DATA_DIR: Path = Path.cwd() / \"experiment_data\"\n",
    "CONSTANT_STEPSIZE: float = 0.1\n",
    "UCB_CONSTANT_STEPSIZE: float = 0.1\n",
    "DISCOUNT_FACTOR: float = 0.1\n",
    "DATE = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "RANDOM_WALK_INTERNAL: int = 1\n",
    "RANDOM_WALK_MACHINE_NUM: int = 1\n",
    "PIECEWISE_INTERNAL: int = 10_000\n",
    "PIECEWISE_METHOD: PiecewizeMethod = PiecewizeMethod.UPSIDE_DOWN\n",
    "DYNAMIC_METHOD: DynamicMethod = DynamicMethod.PIECEWISE\n",
    "\n",
    "if DYNAMIC_METHOD == DynamicMethod.PIECEWISE:\n",
    "    ENV: RLEnv = RLEnv(\n",
    "        machine_count=MACHINE_COUNT,\n",
    "        piecewise_internal=PIECEWISE_INTERNAL,\n",
    "        piecewize_method=PIECEWISE_METHOD,\n",
    "        seed=SEED,\n",
    "    )\n",
    "elif DYNAMIC_METHOD == DynamicMethod.RANDOM_WALK:\n",
    "    ENV: RLEnv = RLEnv(\n",
    "        machine_count=MACHINE_COUNT,\n",
    "        random_walk_internal=RANDOM_WALK_INTERNAL,\n",
    "        random_walk_machine_num=RANDOM_WALK_MACHINE_NUM,\n",
    "        seed=SEED,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Invalid dynamic method: {DYNAMIC_METHOD}\")\n",
    "\n",
    "EPSILON_CONFIG: EpsilonDecreasingConfig = EpsilonDecreasingConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40e091e",
   "metadata": {},
   "source": [
    "# 工厂函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77de71c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_greedy_agent(\n",
    "    env: RLEnv,\n",
    "    epsilon_config: EpsilonDecreasingConfig,\n",
    "    optimistic_init: bool,\n",
    "    optimistic_times: int,\n",
    "    convergence_threshold: float,\n",
    "    convergence_min_steps: int,\n",
    "    constant_stepsize: float,\n",
    "    seed: int,\n",
    ") -> BaseAgent:\n",
    "    return GreedyAgent(\n",
    "        name=GreedyType.GREEDY,\n",
    "        env=env,\n",
    "        algorithm=GreedyAlgorithm(\n",
    "            greedy_type=GreedyType.GREEDY,\n",
    "            optimistic_init=optimistic_init,\n",
    "            optimistic_times=optimistic_times,\n",
    "        ),\n",
    "        epsilon_config=epsilon_config,\n",
    "        convergence_threshold=convergence_threshold,\n",
    "        convergence_min_steps=convergence_min_steps,\n",
    "        constant_stepsize=constant_stepsize,\n",
    "        seed=seed,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d41b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ucb1_agent(\n",
    "    env: RLEnv,\n",
    "    convergence_threshold: float,\n",
    "    convergence_min_steps: int,\n",
    "    constant_stepsize: float,\n",
    "    seed: int,\n",
    ") -> BaseAgent:\n",
    "    return UCBAgent(\n",
    "        name=\"UCB1\",\n",
    "        env=env,\n",
    "        algorithm=UCB1Algorithm(),\n",
    "        convergence_threshold=convergence_threshold,\n",
    "        convergence_min_steps=convergence_min_steps,\n",
    "        constant_stepsize=constant_stepsize,\n",
    "        seed=seed,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "079a4c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ts_agent(\n",
    "    env: RLEnv,\n",
    "    convergence_threshold: float,\n",
    "    convergence_min_steps: int,\n",
    "    discount_factor: float,\n",
    "    seed: int,\n",
    ") -> BaseAgent:\n",
    "    return TSAgent(\n",
    "        name=\"Thompson Sampling\",\n",
    "        env=env,\n",
    "        algorithm=TSAlgorithm(),\n",
    "        convergence_threshold=convergence_threshold,\n",
    "        convergence_min_steps=convergence_min_steps,\n",
    "        discount_factor=discount_factor,\n",
    "        seed=seed,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36db9f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_meta(\n",
    "    run_id: str,\n",
    "    agent_algorithm: str,\n",
    "    is_static: bool,\n",
    ") -> ExperimentMeta:\n",
    "    meta = ExperimentMeta(\n",
    "        run_id=run_id,\n",
    "        run_date=DATE,\n",
    "        total_steps=STEPS,\n",
    "        num_arms=MACHINE_COUNT,\n",
    "        agent_seed=SEED,\n",
    "        agent_algorithm=agent_algorithm,\n",
    "        agent_runs=RUN_COUNT,\n",
    "        convergence_threshold=CONVERGENCE_THRESHOLD,\n",
    "        optimistic_initialization_enabled=ENABLE_OPTIMISTIC,\n",
    "        optimistic_initialization_value=OPTIMISTIC_TIMES,\n",
    "        environment_type=\"static\" if is_static else \"dynamic\",\n",
    "        environment_dynamic_method=DYNAMIC_METHOD.value,\n",
    "        piecewise_stationary_interval=PIECEWISE_INTERNAL,\n",
    "        piecewise_stationary_method=PIECEWISE_METHOD.value,\n",
    "        environment_seed=SEED,\n",
    "        min_convergence_steps=CONVERGENCE_MIN_STEPS,\n",
    "        constant_alpha=CONSTANT_STEPSIZE,\n",
    "        discount_factor=DISCOUNT_FACTOR,\n",
    "        random_walk_interval=RANDOM_WALK_INTERNAL,\n",
    "        random_walk_affected_arms=RANDOM_WALK_MACHINE_NUM,\n",
    "    )\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a0e0bb",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6993f9fc",
   "metadata": {},
   "source": [
    "## 普通贪婪算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e2c02a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100达到收敛时的步数: 100达到收敛时的步数: 130达到收敛时的步数: 100\n",
      "\n",
      "\n",
      "\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 170\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 110\n",
      "达到收敛时的步数: 110\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 120\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 120\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 140\n",
      "达到收敛时的步数: 110\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 120\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 120\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 110\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 110\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 110\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 10000\n",
      "达到收敛时的步数: 140\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 110\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 130\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 10000\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 110\n",
      "达到收敛时的步数: 170\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 150\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 130\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 110\n",
      "达到收敛时的步数: 130\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 190\n",
      "达到收敛时的步数: 110\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 160\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 170\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100达到收敛时的步数: 100\n",
      "\n",
      "达到收敛时的步数: 130\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 200\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 9630\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 140\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 10000\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 110\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 180\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 110\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 830\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 130\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 110\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 110\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 120\n",
      "\n",
      "达到收敛时的步数: 100达到收敛时的步数: 100\n",
      "达到收敛时的步数: 170\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "\n",
      "达到收敛时的步数: 180达到收敛时的步数: 100\n",
      "达到收敛时的步数: 110\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 190\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 110\n",
      "达到收敛时的步数: 160\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "达到收敛时的步数: 100\n",
      "avg_regret=36572.76 avg_regret_rate=0.406364 avg_total_reward=53427.24 avg_optimal_rate=0.16649984 avg_convergence_steps=123.02 avg_convergence_rate=0.402\n",
      "values=[10599.462, 4828.974, 5012.488, 5144.276, 4443.714, 4012.932, 4527.238, 5128.628, 4709.184, 5020.344] counts=[20202.754, 8474.668, 9076.202, 9447.958, 8074.56, 7371.596, 8481.71, 10102.932, 8945.028, 9822.592]\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/20250924_023658/greedy.png\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/20250924_023658/greedy_x_log.png\n",
      "✅ 实验结果数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/20250924_023658/greedy.json\n",
      "✅ 过程数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/20250924_023658/greedyprocess.json\n",
      "909.090909090909\n",
      "0.18181818181818182\n",
      "0.6363636363636364\n",
      "0.7272727272727273\n",
      "0.36363636363636365\n",
      "0.2727272727272727\n",
      "0.45454545454545453\n",
      "0.09090909090909091\n",
      "0.5454545454545454\n",
      "0.9090909090909091\n",
      "0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "run_id = GreedyType.GREEDY.value\n",
    "file_name: Path = EXPERIMENT_DATA_DIR / DATE / f\"{run_id}.png\"\n",
    "if not file_name.exists():\n",
    "    os.makedirs(file_name.parent, exist_ok=True)\n",
    "process_logger = ProcessDataLogger(\n",
    "    run_id=run_id,\n",
    "    total_steps=STEPS,\n",
    "    grid_size=GRID_SIZE,\n",
    ")\n",
    "env = ENV.clone()\n",
    "meta = create_meta(run_id, GreedyType.GREEDY, is_static=True)\n",
    "\n",
    "agents, reward, metrics = batch_train(\n",
    "    count=RUN_COUNT,\n",
    "    agent_factory=create_greedy_agent,\n",
    "    env=env,\n",
    "    epsilon_config=EPSILON_CONFIG,\n",
    "    steps=STEPS,\n",
    "    seed=SEED,\n",
    "    optimistic_init=ENABLE_OPTIMISTIC,\n",
    "    optimistic_times=OPTIMISTIC_TIMES,\n",
    "    convergence_threshold=CONVERGENCE_THRESHOLD,\n",
    "    convergence_min_steps=CONVERGENCE_MIN_STEPS,\n",
    "    process_logger=process_logger,\n",
    "    constant_stepsize=CONSTANT_STEPSIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "print(metrics)\n",
    "print(reward)\n",
    "\n",
    "plot_metrics_history(\n",
    "    agents, f\"贪婪算法 CONSTANT_STEPSIZE= {CONSTANT_STEPSIZE}\", file_name, x_log=False\n",
    ")\n",
    "plot_metrics_history(\n",
    "    agents, f\"贪婪算法 CONSTANT_STEPSIZE= {CONSTANT_STEPSIZE}\", file_name, x_log=True\n",
    ")\n",
    "save_experiment_data(reward, metrics, meta, file_name)\n",
    "process_logger.save(file_name.with_stem(file_name.stem + \"process\"), total_steps=STEPS)\n",
    "dump = process_logger.export(total_steps=STEPS)\n",
    "keys = list(dump.points[0].data.keys())\n",
    "\n",
    "print(ENV.best_reward(1000))\n",
    "for m in ENV.machines:\n",
    "    print(m.reward_probability)\n",
    "\n",
    "clear_var(env, agents, reward, metrics, process_logger, dump)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d0fa0",
   "metadata": {},
   "source": [
    "## UCB1算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "854e802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_regret=5511.868 avg_regret_rate=0.06124297777777778 avg_total_reward=84488.132 avg_optimal_rate=0.32154397999999995 avg_convergence_steps=0.0 avg_convergence_rate=0.0\n",
      "values=[8835.968, 8348.072, 8310.656, 8324.872, 9234.76, 8262.506, 8156.488, 8236.7, 8388.61, 8389.5] counts=[10465.592, 9847.68, 9845.132, 9883.738, 10849.308, 9808.538, 9685.046, 9720.996, 9935.638, 9958.332]\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/20250924_023658/ucb1.png\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/20250924_023658/ucb1_x_log.png\n",
      "✅ 实验结果数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/20250924_023658/ucb1.json\n",
      "✅ 过程数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/20250924_023658/ucb1process.json\n",
      "909.090909090909\n",
      "0.18181818181818182\n",
      "0.6363636363636364\n",
      "0.7272727272727273\n",
      "0.36363636363636365\n",
      "0.2727272727272727\n",
      "0.45454545454545453\n",
      "0.09090909090909091\n",
      "0.5454545454545454\n",
      "0.9090909090909091\n",
      "0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "from ucb1.schemas import UCB1AlgorithmType\n",
    "\n",
    "\n",
    "run_id = UCB1AlgorithmType.UCB1.value\n",
    "file_name: Path = EXPERIMENT_DATA_DIR / DATE / f\"{run_id}.png\"\n",
    "if not file_name.exists():\n",
    "    os.makedirs(file_name.parent, exist_ok=True)\n",
    "process_logger = ProcessDataLogger(\n",
    "    run_id=run_id,\n",
    "    total_steps=STEPS,\n",
    "    grid_size=GRID_SIZE,\n",
    ")\n",
    "env = ENV.clone()\n",
    "meta = create_meta(run_id, UCB1AlgorithmType.UCB1, is_static=True)\n",
    "\n",
    "agents, reward, metrics = batch_train(\n",
    "    count=RUN_COUNT,\n",
    "    agent_factory=create_ucb1_agent,\n",
    "    env=env,\n",
    "    steps=STEPS,\n",
    "    seed=SEED,\n",
    "    convergence_threshold=CONVERGENCE_THRESHOLD,\n",
    "    convergence_min_steps=CONVERGENCE_MIN_STEPS,\n",
    "    process_logger=process_logger,\n",
    "    constant_stepsize=UCB_CONSTANT_STEPSIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "print(metrics)\n",
    "print(reward)\n",
    "\n",
    "plot_metrics_history(\n",
    "    agents,\n",
    "    f\"UCB1 算法 CONSTANT_STEPSIZE= {UCB_CONSTANT_STEPSIZE}\",\n",
    "    file_name,\n",
    "    x_log=False,\n",
    ")\n",
    "plot_metrics_history(\n",
    "    agents,\n",
    "    f\"UCB1 算法 CONSTANT_STEPSIZE= {UCB_CONSTANT_STEPSIZE}\",\n",
    "    file_name,\n",
    "    x_log=True,\n",
    ")\n",
    "save_experiment_data(reward, metrics, meta, file_name)\n",
    "process_logger.save(file_name.with_stem(file_name.stem + \"process\"), total_steps=STEPS)\n",
    "dump = process_logger.export(total_steps=STEPS)\n",
    "keys = list(dump.points[0].data.keys())\n",
    "\n",
    "print(ENV.best_reward(1000))\n",
    "for m in ENV.machines:\n",
    "    print(m.reward_probability)\n",
    "\n",
    "clear_var(env, agents, reward, metrics, process_logger, dump)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9ffd59",
   "metadata": {},
   "source": [
    "## Thompson Sampling 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d294c600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "达到收敛时的步数: 2560\n",
      "达到收敛时的步数: 4110\n",
      "达到收敛时的步数: 6940\n",
      "达到收敛时的步数: 5010\n",
      "达到收敛时的步数: 1450\n",
      "达到收敛时的步数: 8390\n",
      "达到收敛时的步数: 3990\n",
      "达到收敛时的步数: 2550\n",
      "达到收敛时的步数: 4680\n",
      "达到收敛时的步数: 3580\n",
      "达到收敛时的步数: 5640\n",
      "达到收敛时的步数: 3130\n",
      "达到收敛时的步数: 2770\n",
      "达到收敛时的步数: 5240\n",
      "达到收敛时的步数: 6530\n",
      "达到收敛时的步数: 6100\n",
      "达到收敛时的步数: 6460\n",
      "达到收敛时的步数: 1620\n",
      "达到收敛时的步数: 880\n",
      "达到收敛时的步数: 760\n",
      "达到收敛时的步数: 6370\n",
      "达到收敛时的步数: 1620\n",
      "达到收敛时的步数: 1330\n",
      "达到收敛时的步数: 1070\n",
      "达到收敛时的步数: 4810\n",
      "达到收敛时的步数: 9210\n",
      "达到收敛时的步数: 4800\n",
      "达到收敛时的步数: 4210\n",
      "达到收敛时的步数: 8520\n",
      "达到收敛时的步数: 3830\n",
      "达到收敛时的步数: 2190\n",
      "达到收敛时的步数: 7430\n",
      "达到收敛时的步数: 1740\n",
      "达到收敛时的步数: 2350\n",
      "达到收敛时的步数: 2380\n",
      "达到收敛时的步数: 1390\n",
      "达到收敛时的步数: 9220\n",
      "达到收敛时的步数: 6260\n",
      "达到收敛时的步数: 5240\n",
      "达到收敛时的步数: 4700\n",
      "达到收敛时的步数: 1130\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "a <= 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/Jese__Ki/miniconda3/envs/rl_learn/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/Jese__Ki/miniconda3/envs/rl_learn/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n    return list(itertools.starmap(args[0], args[1]))\n  File \"/home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/train.py\", line 179, in _run_single_training\n    _round(\n  File \"/home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/train.py\", line 195, in _round\n    action = agent.act(\n  File \"/home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/thompson_sampling/agent.py\", line 37, in act\n    return self.algorithm.run()\n  File \"/home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/thompson_sampling/agent.py\", line 67, in run\n    return self.ts()\n  File \"/home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/thompson_sampling/agent.py\", line 71, in ts\n    beta = self.agent.rng.beta(r.alpha, r.beta)\n  File \"numpy/random/_generator.pyx\", line 442, in numpy.random._generator.Generator.beta\n  File \"numpy/random/_common.pyx\", line 621, in numpy.random._common.cont\n  File \"numpy/random/_common.pyx\", line 523, in numpy.random._common.cont_broadcast_2\n  File \"numpy/random/_common.pyx\", line 394, in numpy.random._common.check_array_constraint\nValueError: a <= 0\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m env \u001b[38;5;241m=\u001b[39m ENV\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     18\u001b[0m meta \u001b[38;5;241m=\u001b[39m create_meta(run_id, TSAlgorithmType\u001b[38;5;241m.\u001b[39mTS, is_static\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 20\u001b[0m agents, reward, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRUN_COUNT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_ts_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvergence_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONVERGENCE_THRESHOLD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvergence_min_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONVERGENCE_MIN_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocess_logger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess_logger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiscount_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDISCOUNT_FACTOR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_WORKERS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(reward)\n",
      "File \u001b[0;32m~/Projects/learn/Python/rl_atomic/bandit/train.py:88\u001b[0m, in \u001b[0;36mbatch_train\u001b[0;34m(count, agent_factory, env, steps, seed, convergence_threshold, convergence_min_steps, process_logger, num_workers, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m args_list \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     70\u001b[0m     (\n\u001b[1;32m     71\u001b[0m         agent_factory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(count)\n\u001b[1;32m     85\u001b[0m ]\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(processes\u001b[38;5;241m=\u001b[39mnum_workers) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 88\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstarmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_single_training\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# 解包结果：results 是 [(agent1, points1), (agent2, points2), ...]\u001b[39;00m\n\u001b[1;32m     91\u001b[0m agents \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_learn/lib/python3.10/multiprocessing/pool.py:375\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstarmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    370\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_learn/lib/python3.10/multiprocessing/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mValueError\u001b[0m: a <= 0"
     ]
    }
   ],
   "source": [
    "from thompson_sampling.schemas import TSAlgorithmType\n",
    "\n",
    "\n",
    "run_id = TSAlgorithmType.TS.value\n",
    "file_name: Path = EXPERIMENT_DATA_DIR / DATE / f\"{run_id}.png\"\n",
    "if not file_name.exists():\n",
    "    os.makedirs(file_name.parent, exist_ok=True)\n",
    "process_logger = ProcessDataLogger(\n",
    "    run_id=run_id,\n",
    "    total_steps=STEPS,\n",
    "    grid_size=GRID_SIZE,\n",
    ")\n",
    "env = ENV.clone()\n",
    "meta = create_meta(run_id, TSAlgorithmType.TS, is_static=True)\n",
    "\n",
    "agents, reward, metrics = batch_train(\n",
    "    count=RUN_COUNT,\n",
    "    agent_factory=create_ts_agent,\n",
    "    env=env,\n",
    "    steps=STEPS,\n",
    "    seed=SEED,\n",
    "    convergence_threshold=CONVERGENCE_THRESHOLD,\n",
    "    convergence_min_steps=CONVERGENCE_MIN_STEPS,\n",
    "    process_logger=process_logger,\n",
    "    discount_factor=DISCOUNT_FACTOR,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "print(metrics)\n",
    "print(reward)\n",
    "\n",
    "plot_metrics_history(\n",
    "    agents, f\"TS 算法 DISCOUNT_FACTOR= {DISCOUNT_FACTOR}\", file_name, x_log=False\n",
    ")\n",
    "plot_metrics_history(\n",
    "    agents, f\"TS 算法 DISCOUNT_FACTOR= {DISCOUNT_FACTOR}\", file_name, x_log=True\n",
    ")\n",
    "save_experiment_data(reward, metrics, meta, file_name)\n",
    "process_logger.save(file_name.with_stem(file_name.stem + \"process\"), total_steps=STEPS)\n",
    "dump = process_logger.export(total_steps=STEPS)\n",
    "keys = list(dump.points[0].data.keys())\n",
    "\n",
    "print(ENV.best_reward(1000))\n",
    "for m in ENV.machines:\n",
    "    print(m.reward_probability)\n",
    "\n",
    "clear_var(env, agents, reward, metrics, process_logger, dump)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
