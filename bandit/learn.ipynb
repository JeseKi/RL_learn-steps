{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "854eb06c",
   "metadata": {},
   "source": [
    "# 导入必要库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1fb3304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import gc\n",
    "import time\n",
    "\n",
    "from core import RLEnv\n",
    "from core.agent import BaseAgent\n",
    "from greedy import (\n",
    "    EpsilonDecreasingConfig,\n",
    "    GreedyAgent,\n",
    "    greedy_average,\n",
    "    epsilon_average,\n",
    "    epsilon_decreasing_average,\n",
    ")\n",
    "from ucb1 import UCBAgent, ucb1\n",
    "from thompson_sampling import TSAgent\n",
    "\n",
    "from train import batch_train\n",
    "from utils import plot_metrics_history, save_experiment_data, ProcessDataLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c2d22f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS: int = 100_000\n",
    "GRID_SIZE: int = 500\n",
    "\n",
    "SEED: int = 42\n",
    "MACHINE_COUNT: int = 10\n",
    "RUN_COUNT: int = 50\n",
    "CONVERGENCE_THRESHOLD: float = 0.9\n",
    "CONVERGENCE_MIN_STEPS: int = 100\n",
    "OPTIMISTIC_TIMES: int = 1\n",
    "ENABLE_OPTIMISTIC: bool = True\n",
    "EXPERIMENT_DATA_DIR: Path = Path.cwd() / \"experiment_data\"\n",
    "\n",
    "ENV: RLEnv = RLEnv(machine_count=MACHINE_COUNT, seed=SEED)\n",
    "EPSILON_CONFIG: EpsilonDecreasingConfig = EpsilonDecreasingConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40e091e",
   "metadata": {},
   "source": [
    "# 工厂函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f02e2844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_id(agent_name: str) -> str:\n",
    "    return agent_name + \"_\" + str(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77de71c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_greedy_agent(\n",
    "    env: RLEnv,\n",
    "    epsilon_config: EpsilonDecreasingConfig,\n",
    "    optimistic_init: bool,\n",
    "    optimistic_times: int,\n",
    "    convergence_threshold: float,\n",
    "    convergence_min_steps: int,\n",
    "    seed: int,\n",
    ") -> BaseAgent:\n",
    "    return GreedyAgent(\n",
    "        name=greedy_average.__name__,\n",
    "        env=env,\n",
    "        greedy_algorithm=greedy_average,\n",
    "        epsilon_config=epsilon_config,\n",
    "        optimistic_init=optimistic_init,\n",
    "        optimistic_times=optimistic_times,\n",
    "        convergence_threshold=convergence_threshold,\n",
    "        convergence_min_steps=convergence_min_steps,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_epsilon_agent(\n",
    "    env: RLEnv,\n",
    "    epsilon_config: EpsilonDecreasingConfig,\n",
    "    optimistic_init: bool,\n",
    "    optimistic_times: int,\n",
    "    convergence_threshold: float,\n",
    "    convergence_min_steps: int,\n",
    "    seed: int,\n",
    ") -> BaseAgent:\n",
    "    return GreedyAgent(\n",
    "        name=epsilon_average.__name__,\n",
    "        env=env,\n",
    "        greedy_algorithm=epsilon_average,\n",
    "        epsilon_config=epsilon_config,\n",
    "        optimistic_init=optimistic_init,\n",
    "        optimistic_times=optimistic_times,\n",
    "        convergence_threshold=convergence_threshold,\n",
    "        convergence_min_steps=convergence_min_steps,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_decreasing_agent(\n",
    "    env: RLEnv,\n",
    "    epsilon_config: EpsilonDecreasingConfig,\n",
    "    optimistic_init: bool,\n",
    "    optimistic_times: int,\n",
    "    convergence_threshold: float,\n",
    "    convergence_min_steps: int,\n",
    "    seed: int,\n",
    ") -> BaseAgent:\n",
    "    return GreedyAgent(\n",
    "        name=epsilon_decreasing_average.__name__,\n",
    "        env=env,\n",
    "        greedy_algorithm=epsilon_decreasing_average,\n",
    "        epsilon_config=epsilon_config,\n",
    "        optimistic_init=optimistic_init,\n",
    "        optimistic_times=optimistic_times,\n",
    "        convergence_threshold=convergence_threshold,\n",
    "        convergence_min_steps=convergence_min_steps,\n",
    "        seed=seed,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d41b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ucb1_agent(\n",
    "    env: RLEnv,\n",
    "    convergence_threshold: float,\n",
    "    convergence_min_steps: int,\n",
    "    seed: int,\n",
    ") -> BaseAgent:\n",
    "    return UCBAgent(\n",
    "        name=ucb1.__name__,\n",
    "        env=env,\n",
    "        ucb1_algorithm=ucb1,\n",
    "        convergence_threshold=convergence_threshold,\n",
    "        convergence_min_steps=convergence_min_steps,\n",
    "        seed=seed,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "079a4c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ts_agent(\n",
    "    env: RLEnv,\n",
    "    convergence_threshold: float,\n",
    "    convergence_min_steps: int,\n",
    "    seed: int,\n",
    ") -> BaseAgent:\n",
    "    return TSAgent(\n",
    "        name=TSAgent.__name__,\n",
    "        env=env,\n",
    "        convergence_threshold=convergence_threshold,\n",
    "        convergence_min_steps=convergence_min_steps,\n",
    "        seed=seed,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a0e0bb",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6993f9fc",
   "metadata": {},
   "source": [
    "## 普通贪婪算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f976af58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "达到收敛时的步数: 190\n",
      "达到收敛时的步数: 510\n",
      "达到收敛时的步数: 250\n",
      "达到收敛时的步数: 130\n",
      "达到收敛时的步数: 560\n",
      "达到收敛时的步数: 150\n",
      "达到收敛时的步数: 160\n",
      "达到收敛时的步数: 170\n",
      "达到收敛时的步数: 150\n",
      "达到收敛时的步数: 200\n",
      "达到收敛时的步数: 230\n",
      "达到收敛时的步数: 220\n",
      "达到收敛时的步数: 320\n",
      "达到收敛时的步数: 370\n",
      "达到收敛时的步数: 350\n",
      "达到收敛时的步数: 700\n",
      "达到收敛时的步数: 250\n",
      "达到收敛时的步数: 240\n",
      "达到收敛时的步数: 410\n",
      "达到收敛时的步数: 220\n",
      "达到收敛时的步数: 210\n",
      "达到收敛时的步数: 230\n",
      "达到收敛时的步数: 1250\n",
      "达到收敛时的步数: 130\n",
      "达到收敛时的步数: 220\n",
      "达到收敛时的步数: 510\n",
      "达到收敛时的步数: 180\n",
      "达到收敛时的步数: 480\n",
      "达到收敛时的步数: 380\n",
      "达到收敛时的步数: 430\n",
      "达到收敛时的步数: 130\n",
      "达到收敛时的步数: 240\n",
      "达到收敛时的步数: 170\n",
      "达到收敛时的步数: 220\n",
      "达到收敛时的步数: 320\n",
      "达到收敛时的步数: 200\n",
      "avg_regret=3637.7109090909116 avg_regret_rate=0.04001482000000005 avg_total_reward=87271.38 avg_optimal_rate=0.7197880000000002 avg_convergence_steps=221.6 avg_convergence_rate=0.72\n",
      "values=[0.14, 0.26, 0.5, 0.7, 0.98, 3.5, 1273.28, 5815.76, 14737.92, 65438.34] counts=[1.14, 1.26, 1.54, 1.7, 2.0, 6.58, 2004.28, 8000.8, 18001.9, 71978.8]\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/greedy_average_1757844598.1695883_T=100000_K=10_Q_0=1.png\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/greedy_average_1757844598.1695883_T=100000_K=10_Q_0=1_x_log.png\n",
      "✅ 实验结果数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/greedy_average_1757844598.1695883_T=100000_K=10_Q_0=1.json\n",
      "✅ 过程数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/greedy_average_1757844598.1695883_T=100000_K=10_Q_0=1process.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id = get_run_id(greedy_average.__name__)\n",
    "file_name: Path = (\n",
    "    EXPERIMENT_DATA_DIR\n",
    "    / f\"{run_id}_T={STEPS}_K={MACHINE_COUNT}_Q_0={OPTIMISTIC_TIMES}.png\"\n",
    ")\n",
    "process_logger = ProcessDataLogger(\n",
    "    run_id=run_id,\n",
    "    total_steps=STEPS,\n",
    "    grid_size=GRID_SIZE,\n",
    ")\n",
    "\n",
    "agents, reward, metrics = batch_train(\n",
    "    count=RUN_COUNT,\n",
    "    agent_factory=create_greedy_agent,\n",
    "    env=ENV,\n",
    "    epsilon_config=EPSILON_CONFIG,\n",
    "    steps=STEPS,\n",
    "    seed=SEED,\n",
    "    optimistic_init=ENABLE_OPTIMISTIC,\n",
    "    optimistic_times=OPTIMISTIC_TIMES,\n",
    "    convergence_threshold=CONVERGENCE_THRESHOLD,\n",
    "    convergence_min_steps=CONVERGENCE_MIN_STEPS,\n",
    "    process_logger=process_logger,\n",
    ")\n",
    "print(metrics)\n",
    "print(reward)\n",
    "\n",
    "plot_metrics_history(agents, \"贪婪算法\", file_name, x_log=False)\n",
    "plot_metrics_history(agents, \"贪婪算法\", file_name, x_log=True)\n",
    "save_experiment_data(reward, metrics, file_name)\n",
    "process_logger.save(file_name.with_stem(file_name.stem + \"process\"), total_steps=STEPS)\n",
    "dump = process_logger.export(total_steps=STEPS)\n",
    "keys = list(dump.points[0].data.keys())\n",
    "\n",
    "del agents, reward, metrics, process_logger, dump\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d0fa0",
   "metadata": {},
   "source": [
    "## UCB1算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "854e802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "达到收敛时的步数: 27370\n",
      "达到收敛时的步数: 22770\n",
      "达到收敛时的步数: 23020\n",
      "达到收敛时的步数: 21450\n",
      "达到收敛时的步数: 22200\n",
      "达到收敛时的步数: 26610\n",
      "达到收敛时的步数: 33250\n",
      "达到收敛时的步数: 26910\n",
      "达到收敛时的步数: 24690\n",
      "达到收敛时的步数: 24780\n",
      "达到收敛时的步数: 25700\n",
      "达到收敛时的步数: 18460\n",
      "达到收敛时的步数: 23190\n",
      "达到收敛时的步数: 20900\n",
      "达到收敛时的步数: 19750\n",
      "达到收敛时的步数: 23520\n",
      "达到收敛时的步数: 21410\n",
      "达到收敛时的步数: 22240\n",
      "达到收敛时的步数: 21660\n",
      "达到收敛时的步数: 33440\n",
      "达到收敛时的步数: 25870\n",
      "达到收敛时的步数: 29490\n",
      "达到收敛时的步数: 29340\n",
      "达到收敛时的步数: 27550\n",
      "达到收敛时的步数: 29970\n",
      "达到收敛时的步数: 27540\n",
      "达到收敛时的步数: 26810\n",
      "达到收敛时的步数: 23640\n",
      "达到收敛时的步数: 27110\n",
      "达到收敛时的步数: 26530\n",
      "达到收敛时的步数: 22260\n",
      "达到收敛时的步数: 21750\n",
      "达到收敛时的步数: 21820\n",
      "达到收敛时的步数: 24530\n",
      "达到收敛时的步数: 24690\n",
      "达到收敛时的步数: 17980\n",
      "达到收敛时的步数: 23050\n",
      "达到收敛时的步数: 22190\n",
      "达到收敛时的步数: 19060\n",
      "达到收敛时的步数: 27850\n",
      "达到收敛时的步数: 22380\n",
      "达到收敛时的步数: 30000\n",
      "达到收敛时的步数: 21180\n",
      "达到收敛时的步数: 27130\n",
      "达到收敛时的步数: 27530\n",
      "达到收敛时的步数: 22220\n",
      "达到收敛时的步数: 26750\n",
      "达到收敛时的步数: 25470\n",
      "达到收敛时的步数: 25470\n",
      "达到收敛时的步数: 24730\n",
      "avg_regret=614.9509090909118 avg_regret_rate=0.006764460000000031 avg_total_reward=90294.14 avg_optimal_rate=0.9657229999999997 avg_convergence_steps=24704.2 avg_convergence_rate=1.0\n",
      "values=[2.74, 6.96, 13.58, 26.1, 45.56, 85.2, 178.82, 443.8, 1705.74, 87785.64] counts=[33.24, 40.96, 52.38, 72.52, 101.66, 157.28, 280.28, 607.84, 2081.54, 96572.3]\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/ucb1_1757844609.966043_T=100000_K=10_Q_0=1.png\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/ucb1_1757844609.966043_T=100000_K=10_Q_0=1_x_log.png\n",
      "✅ 实验结果数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/ucb1_1757844609.966043_T=100000_K=10_Q_0=1.json\n",
      "✅ 过程数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/ucb1_1757844609.966043_T=100000_K=10_Q_0=1process.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id = get_run_id(ucb1.__name__)\n",
    "file_name: Path = (\n",
    "    EXPERIMENT_DATA_DIR\n",
    "    / f\"{run_id}_T={STEPS}_K={MACHINE_COUNT}_Q_0={OPTIMISTIC_TIMES}.png\"\n",
    ")\n",
    "process_logger = ProcessDataLogger(\n",
    "    run_id=run_id,\n",
    "    total_steps=STEPS,\n",
    "    grid_size=GRID_SIZE,\n",
    ")\n",
    "\n",
    "agents, reward, metrics = batch_train(\n",
    "    count=RUN_COUNT,\n",
    "    agent_factory=create_ucb1_agent,\n",
    "    env=ENV,\n",
    "    steps=STEPS,\n",
    "    seed=SEED,\n",
    "    convergence_threshold=CONVERGENCE_THRESHOLD,\n",
    "    convergence_min_steps=CONVERGENCE_MIN_STEPS,\n",
    "    process_logger=process_logger,\n",
    ")\n",
    "print(metrics)\n",
    "print(reward)\n",
    "\n",
    "plot_metrics_history(agents, \"UCB1 算法\", file_name, x_log=False)\n",
    "plot_metrics_history(agents, \"UCB1 算法\", file_name, x_log=True)\n",
    "save_experiment_data(reward, metrics, file_name)\n",
    "process_logger.save(file_name.with_stem(file_name.stem + \"process\"), total_steps=STEPS)\n",
    "dump = process_logger.export(total_steps=STEPS)\n",
    "keys = list(dump.points[0].data.keys())\n",
    "\n",
    "del agents, reward, metrics, process_logger, dump\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9ffd59",
   "metadata": {},
   "source": [
    "# Thompson Sampling 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d294c600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "达到收敛时的步数: 1050\n",
      "达到收敛时的步数: 610\n",
      "达到收敛时的步数: 990\n",
      "达到收敛时的步数: 330\n",
      "达到收敛时的步数: 1260\n",
      "达到收敛时的步数: 620\n",
      "达到收敛时的步数: 4460\n",
      "达到收敛时的步数: 950\n",
      "达到收敛时的步数: 4050\n",
      "达到收敛时的步数: 870\n",
      "达到收敛时的步数: 1410\n",
      "达到收敛时的步数: 1200\n",
      "达到收敛时的步数: 1390\n",
      "达到收敛时的步数: 980\n",
      "达到收敛时的步数: 580\n",
      "达到收敛时的步数: 1020\n",
      "达到收敛时的步数: 460\n",
      "达到收敛时的步数: 1120\n",
      "达到收敛时的步数: 440\n",
      "达到收敛时的步数: 1440\n",
      "达到收敛时的步数: 770\n",
      "达到收敛时的步数: 800\n",
      "达到收敛时的步数: 1510\n",
      "达到收敛时的步数: 2290\n",
      "达到收敛时的步数: 2000\n",
      "达到收敛时的步数: 630\n",
      "达到收敛时的步数: 1270\n",
      "达到收敛时的步数: 6610\n",
      "达到收敛时的步数: 1140\n",
      "达到收敛时的步数: 4070\n",
      "达到收敛时的步数: 1010\n",
      "达到收敛时的步数: 1450\n",
      "达到收敛时的步数: 320\n",
      "达到收敛时的步数: 2030\n",
      "达到收敛时的步数: 1240\n",
      "达到收敛时的步数: 3430\n",
      "达到收敛时的步数: 1110\n",
      "达到收敛时的步数: 1370\n",
      "达到收敛时的步数: 930\n",
      "达到收敛时的步数: 2180\n",
      "达到收敛时的步数: 1100\n",
      "达到收敛时的步数: 2190\n",
      "达到收敛时的步数: 760\n",
      "达到收敛时的步数: 540\n",
      "达到收敛时的步数: 1170\n",
      "达到收敛时的步数: 2280\n",
      "达到收敛时的步数: 480\n",
      "达到收敛时的步数: 550\n",
      "达到收敛时的步数: 2590\n",
      "达到收敛时的步数: 350\n",
      "avg_regret=49.170909090911735 avg_regret_rate=0.0005408800000000293 avg_total_reward=90859.92 avg_optimal_rate=0.9970712 avg_convergence_steps=1468.0 avg_convergence_rate=1.0\n",
      "values=[0.42, 1.16, 2.58, 3.14, 4.7, 8.66, 17.64, 32.24, 135.2, 90654.18] counts=[5.06, 6.3, 8.36, 9.04, 11.14, 16.38, 27.48, 44.96, 164.16, 99707.12]\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/thompson_sampling_1757844651.7483723_T=100000_K=10_Q_0=1.png\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/thompson_sampling_1757844651.7483723_T=100000_K=10_Q_0=1_x_log.png\n",
      "✅ 实验结果数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/thompson_sampling_1757844651.7483723_T=100000_K=10_Q_0=1.json\n",
      "✅ 过程数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/thompson_sampling_1757844651.7483723_T=100000_K=10_Q_0=1process.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "228763"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id = get_run_id(\"thompson_sampling\")\n",
    "file_name: Path = (\n",
    "    EXPERIMENT_DATA_DIR\n",
    "    / f\"{run_id}_T={STEPS}_K={MACHINE_COUNT}_Q_0={OPTIMISTIC_TIMES}.png\"\n",
    ")\n",
    "process_logger = ProcessDataLogger(\n",
    "    run_id=run_id,\n",
    "    total_steps=STEPS,\n",
    "    grid_size=GRID_SIZE,\n",
    ")\n",
    "\n",
    "agents, reward, metrics = batch_train(\n",
    "    count=RUN_COUNT,\n",
    "    agent_factory=create_ts_agent,\n",
    "    env=ENV,\n",
    "    steps=STEPS,\n",
    "    seed=SEED,\n",
    "    convergence_threshold=CONVERGENCE_THRESHOLD,\n",
    "    convergence_min_steps=CONVERGENCE_MIN_STEPS,\n",
    "    process_logger=process_logger,\n",
    ")\n",
    "print(metrics)\n",
    "print(reward)\n",
    "\n",
    "plot_metrics_history(agents, \"TS 算法\", file_name, x_log=False)\n",
    "plot_metrics_history(agents, \"TS 算法\", file_name, x_log=True)\n",
    "save_experiment_data(reward, metrics, file_name)\n",
    "process_logger.save(file_name.with_stem(file_name.stem + \"process\"), total_steps=STEPS)\n",
    "dump = process_logger.export(total_steps=STEPS)\n",
    "keys = list(dump.points[0].data.keys())\n",
    "\n",
    "del agents, reward, metrics, process_logger, dump\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
