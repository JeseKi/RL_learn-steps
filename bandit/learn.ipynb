{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "854eb06c",
   "metadata": {},
   "source": [
    "# 导入必要库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1fb3304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "from core import RLEnv\n",
    "from core import BaseAgent\n",
    "from core.schemas import PiecewizeMethod  # noqa\n",
    "from greedy import EpsilonDecreasingConfig, GreedyAgent, GreedyAlgorithm, GreedyType\n",
    "from ucb1 import UCBAgent, UCB1Algorithm\n",
    "from thompson_sampling import TSAgent, TSAlgorithm\n",
    "\n",
    "from train import batch_train\n",
    "from utils import plot_metrics_history, save_experiment_data, ProcessDataLogger, clear_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c2d22f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS: int = 100_000\n",
    "GRID_SIZE: int = 500\n",
    "\n",
    "SEED: int = 42\n",
    "MACHINE_COUNT: int = 10\n",
    "RUN_COUNT: int = 5\n",
    "CONVERGENCE_THRESHOLD: float = 0.9\n",
    "CONVERGENCE_MIN_STEPS: int = 100\n",
    "OPTIMISTIC_TIMES: int = 1\n",
    "ENABLE_OPTIMISTIC: bool = True\n",
    "EXPERIMENT_DATA_DIR: Path = Path.cwd() / \"experiment_data\"\n",
    "CONSTANT_STEPSIZE: float = 0\n",
    "DISCOUNT_FACTOR: float = 0\n",
    "\n",
    "ENV: RLEnv = RLEnv(\n",
    "    machine_count=MACHINE_COUNT,\n",
    "    # random_walk_internal=1,\n",
    "    # random_walk_machine_num=1,\n",
    "    # piecewise_internal=1,\n",
    "    # piecewize_method=PiecewizeMethod.UPSIDE_DOWN,\n",
    "    seed=SEED,\n",
    ")\n",
    "EPSILON_CONFIG: EpsilonDecreasingConfig = EpsilonDecreasingConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40e091e",
   "metadata": {},
   "source": [
    "# 工厂函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f02e2844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_id(agent_name: str) -> str:\n",
    "    return agent_name + \"_\" + str(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77de71c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_greedy_agent(\n",
    "    env: RLEnv,\n",
    "    epsilon_config: EpsilonDecreasingConfig,\n",
    "    optimistic_init: bool,\n",
    "    optimistic_times: int,\n",
    "    convergence_threshold: float,\n",
    "    convergence_min_steps: int,\n",
    "    constant_stepsize: float,\n",
    "    seed: int,\n",
    ") -> BaseAgent:\n",
    "    return GreedyAgent(\n",
    "        name=GreedyType.GREEDY,\n",
    "        env=env,\n",
    "        algorithm=GreedyAlgorithm(\n",
    "            greedy_type=GreedyType.GREEDY,\n",
    "            optimistic_init=optimistic_init,\n",
    "            optimistic_times=optimistic_times,\n",
    "        ),\n",
    "        epsilon_config=epsilon_config,\n",
    "        convergence_threshold=convergence_threshold,\n",
    "        convergence_min_steps=convergence_min_steps,\n",
    "        constant_stepsize=constant_stepsize,\n",
    "        seed=seed,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d41b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ucb1_agent(\n",
    "    env: RLEnv,\n",
    "    convergence_threshold: float,\n",
    "    convergence_min_steps: int,\n",
    "    constant_stepsize: float,\n",
    "    seed: int,\n",
    ") -> BaseAgent:\n",
    "    return UCBAgent(\n",
    "        name=\"UCB1\",\n",
    "        env=env,\n",
    "        algorithm=UCB1Algorithm(),\n",
    "        convergence_threshold=convergence_threshold,\n",
    "        convergence_min_steps=convergence_min_steps,\n",
    "        constant_stepsize=constant_stepsize,\n",
    "        seed=seed,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "079a4c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ts_agent(\n",
    "    env: RLEnv,\n",
    "    convergence_threshold: float,\n",
    "    convergence_min_steps: int,\n",
    "    discount_factor: float,\n",
    "    seed: int,\n",
    ") -> BaseAgent:\n",
    "    return TSAgent(\n",
    "        name=\"Thompson Sampling\",\n",
    "        env=env,\n",
    "        algorithm=TSAlgorithm(),\n",
    "        convergence_threshold=convergence_threshold,\n",
    "        convergence_min_steps=convergence_min_steps,\n",
    "        discount_factor=discount_factor,\n",
    "        seed=seed,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a0e0bb",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6993f9fc",
   "metadata": {},
   "source": [
    "## 普通贪婪算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f976af58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "达到收敛时的步数: 310\n",
      "达到收敛时的步数: 280\n",
      "达到收敛时的步数: 300\n",
      "avg_regret=3688.6909090909116 avg_regret_rate=0.04057560000000003 avg_total_reward=87220.4 avg_optimal_rate=0.599844 avg_convergence_steps=178.0 avg_convergence_rate=0.6\n",
      "values=[0.6, 4.4, 17.8, 0.8, 0.6, 0.8, 0.4, 0.8, 54495.4, 32698.8] counts=[1.6, 6.4, 23.8, 1.8, 1.6, 1.8, 1.4, 1.8, 59984.4, 39975.4]\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/greedy_1758175778.051391_T=100000_K=10_Q_0=1.png\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/greedy_1758175778.051391_T=100000_K=10_Q_0=1_x_log.png\n",
      "✅ 实验结果数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/greedy_1758175778.051391_T=100000_K=10_Q_0=1.json\n",
      "✅ 过程数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/greedy_1758175778.051391_T=100000_K=10_Q_0=1process.json\n",
      "909.090909090909\n",
      "0.18181818181818182\n",
      "0.6363636363636364\n",
      "0.7272727272727273\n",
      "0.36363636363636365\n",
      "0.2727272727272727\n",
      "0.45454545454545453\n",
      "0.09090909090909091\n",
      "0.5454545454545454\n",
      "0.9090909090909091\n",
      "0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "run_id = get_run_id(GreedyType.GREEDY)\n",
    "file_name: Path = (\n",
    "    EXPERIMENT_DATA_DIR\n",
    "    / f\"{run_id}_T={STEPS}_K={MACHINE_COUNT}_Q_0={OPTIMISTIC_TIMES}.png\"\n",
    ")\n",
    "process_logger = ProcessDataLogger(\n",
    "    run_id=run_id,\n",
    "    total_steps=STEPS,\n",
    "    grid_size=GRID_SIZE,\n",
    ")\n",
    "env = ENV.clone()\n",
    "\n",
    "agents, reward, metrics = batch_train(\n",
    "    count=RUN_COUNT,\n",
    "    agent_factory=create_greedy_agent,\n",
    "    env=env,\n",
    "    epsilon_config=EPSILON_CONFIG,\n",
    "    steps=STEPS,\n",
    "    seed=SEED,\n",
    "    optimistic_init=ENABLE_OPTIMISTIC,\n",
    "    optimistic_times=OPTIMISTIC_TIMES,\n",
    "    convergence_threshold=CONVERGENCE_THRESHOLD,\n",
    "    convergence_min_steps=CONVERGENCE_MIN_STEPS,\n",
    "    process_logger=process_logger,\n",
    "    constant_stepsize=CONSTANT_STEPSIZE,\n",
    ")\n",
    "print(metrics)\n",
    "print(reward)\n",
    "\n",
    "plot_metrics_history(agents, \"贪婪算法\", file_name, x_log=False)\n",
    "plot_metrics_history(agents, \"贪婪算法\", file_name, x_log=True)\n",
    "save_experiment_data(reward, metrics, file_name)\n",
    "process_logger.save(file_name.with_stem(file_name.stem + \"process\"), total_steps=STEPS)\n",
    "dump = process_logger.export(total_steps=STEPS)\n",
    "keys = list(dump.points[0].data.keys())\n",
    "\n",
    "print(ENV.best_reward(1000))\n",
    "for m in ENV.machines:\n",
    "    print(m.reward_probability)\n",
    "\n",
    "clear_var(env, agents, reward, metrics, process_logger, dump)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d0fa0",
   "metadata": {},
   "source": [
    "## UCB1算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "854e802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_regret=72681.09090909091 avg_regret_rate=0.7994920000000001 avg_total_reward=18228.0 avg_optimal_rate=0.0 avg_convergence_steps=0.0 avg_convergence_rate=0.0\n",
      "values=[18228.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] counts=[100000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/ucb1_1758175780.738071_T=100000_K=10_Q_0=1.png\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/ucb1_1758175780.738071_T=100000_K=10_Q_0=1_x_log.png\n",
      "✅ 实验结果数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/ucb1_1758175780.738071_T=100000_K=10_Q_0=1.json\n",
      "✅ 过程数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/ucb1_1758175780.738071_T=100000_K=10_Q_0=1process.json\n",
      "909.090909090909\n",
      "0.18181818181818182\n",
      "0.6363636363636364\n",
      "0.7272727272727273\n",
      "0.36363636363636365\n",
      "0.2727272727272727\n",
      "0.45454545454545453\n",
      "0.09090909090909091\n",
      "0.5454545454545454\n",
      "0.9090909090909091\n",
      "0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "run_id = get_run_id(\"ucb1\")\n",
    "file_name: Path = (\n",
    "    EXPERIMENT_DATA_DIR\n",
    "    / f\"{run_id}_T={STEPS}_K={MACHINE_COUNT}_Q_0={OPTIMISTIC_TIMES}.png\"\n",
    ")\n",
    "process_logger = ProcessDataLogger(\n",
    "    run_id=run_id,\n",
    "    total_steps=STEPS,\n",
    "    grid_size=GRID_SIZE,\n",
    ")\n",
    "env = ENV.clone()\n",
    "\n",
    "agents, reward, metrics = batch_train(\n",
    "    count=RUN_COUNT,\n",
    "    agent_factory=create_ucb1_agent,\n",
    "    env=env,\n",
    "    steps=STEPS,\n",
    "    seed=SEED,\n",
    "    convergence_threshold=CONVERGENCE_THRESHOLD,\n",
    "    convergence_min_steps=CONVERGENCE_MIN_STEPS,\n",
    "    process_logger=process_logger,\n",
    "    constant_stepsize=CONSTANT_STEPSIZE,\n",
    ")\n",
    "print(metrics)\n",
    "print(reward)\n",
    "\n",
    "plot_metrics_history(agents, \"UCB1 算法\", file_name, x_log=False)\n",
    "plot_metrics_history(agents, \"UCB1 算法\", file_name, x_log=True)\n",
    "save_experiment_data(reward, metrics, file_name)\n",
    "process_logger.save(file_name.with_stem(file_name.stem + \"process\"), total_steps=STEPS)\n",
    "dump = process_logger.export(total_steps=STEPS)\n",
    "keys = list(dump.points[0].data.keys())\n",
    "\n",
    "print(ENV.best_reward(1000))\n",
    "for m in ENV.machines:\n",
    "    print(m.reward_probability)\n",
    "\n",
    "clear_var(env, agents, reward, metrics, process_logger, dump)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9ffd59",
   "metadata": {},
   "source": [
    "# Thompson Sampling 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d294c600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "达到收敛时的步数: 1810\n",
      "达到收敛时的步数: 1550\n",
      "达到收敛时的步数: 960\n",
      "达到收敛时的步数: 1110\n",
      "达到收敛时的步数: 730\n",
      "avg_regret=78.69090909091173 avg_regret_rate=0.0008656000000000291 avg_total_reward=90830.4 avg_optimal_rate=0.9967940000000001 avg_convergence_steps=1232.0 avg_convergence_rate=1.0\n",
      "values=[1.0, 26.2, 34.0, 3.4, 1.8, 8.4, 0.6, 10.0, 90602.6, 142.4] counts=[6.0, 38.4, 46.4, 9.4, 7.2, 16.6, 6.0, 17.2, 99679.4, 173.4]\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/thompson_sampling_1758175783.2109652_T=100000_K=10_Q_0=1.png\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/thompson_sampling_1758175783.2109652_T=100000_K=10_Q_0=1_x_log.png\n",
      "✅ 实验结果数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/thompson_sampling_1758175783.2109652_T=100000_K=10_Q_0=1.json\n",
      "✅ 过程数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/thompson_sampling_1758175783.2109652_T=100000_K=10_Q_0=1process.json\n",
      "909.090909090909\n",
      "0.18181818181818182\n",
      "0.6363636363636364\n",
      "0.7272727272727273\n",
      "0.36363636363636365\n",
      "0.2727272727272727\n",
      "0.45454545454545453\n",
      "0.09090909090909091\n",
      "0.5454545454545454\n",
      "0.9090909090909091\n",
      "0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "run_id = get_run_id(\"thompson_sampling\")\n",
    "file_name: Path = (\n",
    "    EXPERIMENT_DATA_DIR\n",
    "    / f\"{run_id}_T={STEPS}_K={MACHINE_COUNT}_Q_0={OPTIMISTIC_TIMES}.png\"\n",
    ")\n",
    "process_logger = ProcessDataLogger(\n",
    "    run_id=run_id,\n",
    "    total_steps=STEPS,\n",
    "    grid_size=GRID_SIZE,\n",
    ")\n",
    "env = ENV.clone()\n",
    "\n",
    "agents, reward, metrics = batch_train(\n",
    "    count=RUN_COUNT,\n",
    "    agent_factory=create_ts_agent,\n",
    "    env=env,\n",
    "    steps=STEPS,\n",
    "    seed=SEED,\n",
    "    convergence_threshold=CONVERGENCE_THRESHOLD,\n",
    "    convergence_min_steps=CONVERGENCE_MIN_STEPS,\n",
    "    process_logger=process_logger,\n",
    "    discount_factor=DISCOUNT_FACTOR,\n",
    ")\n",
    "print(metrics)\n",
    "print(reward)\n",
    "\n",
    "plot_metrics_history(agents, \"TS 算法\", file_name, x_log=False)\n",
    "plot_metrics_history(agents, \"TS 算法\", file_name, x_log=True)\n",
    "save_experiment_data(reward, metrics, file_name)\n",
    "process_logger.save(file_name.with_stem(file_name.stem + \"process\"), total_steps=STEPS)\n",
    "dump = process_logger.export(total_steps=STEPS)\n",
    "keys = list(dump.points[0].data.keys())\n",
    "\n",
    "print(ENV.best_reward(1000))\n",
    "for m in ENV.machines:\n",
    "    print(m.reward_probability)\n",
    "\n",
    "clear_var(env, agents, reward, metrics, process_logger, dump)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
