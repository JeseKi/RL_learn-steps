{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "854eb06c",
   "metadata": {},
   "source": [
    "# 导入必要库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1fb3304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import gc\n",
    "import time\n",
    "\n",
    "from core import RLEnv\n",
    "from core.agent import BaseAgent\n",
    "from greedy import (\n",
    "    EpsilonDecreasingConfig,\n",
    "    GreedyAgent,\n",
    "    greedy_average,\n",
    "    epsilon_average,\n",
    "    epsilon_decreasing_average,\n",
    ")\n",
    "from ucb1 import UCBAgent, ucb1\n",
    "from thompson_sampling import TSAgent\n",
    "\n",
    "from train import batch_train\n",
    "from utils import plot_metrics_history, save_experiment_data, ProcessDataLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c2d22f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS: int = 100_000\n",
    "GRID_SIZE: int = 500\n",
    "\n",
    "SEED: int = 42\n",
    "MACHINE_COUNT: int = 10\n",
    "RUN_COUNT: int = 50\n",
    "CONVERGENCE_THRESHOLD: float = 0.9\n",
    "CONVERGENCE_MIN_STEPS: int = 100\n",
    "OPTIMISTIC_TIMES: int = 1\n",
    "ENABLE_OPTIMISTIC: bool = True\n",
    "EXPERIMENT_DATA_DIR: Path = Path.cwd() / \"experiment_data\"\n",
    "\n",
    "ENV: RLEnv = RLEnv(machine_count=MACHINE_COUNT, seed=SEED)\n",
    "EPSILON_CONFIG: EpsilonDecreasingConfig = EpsilonDecreasingConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40e091e",
   "metadata": {},
   "source": [
    "# 工厂函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f02e2844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_id(agent_name: str) -> str:\n",
    "    return agent_name + \"_\" + str(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77de71c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_greedy_agent(\n",
    "    env: RLEnv,\n",
    "    epsilon_config: EpsilonDecreasingConfig,\n",
    "    optimistic_init: bool,\n",
    "    optimistic_times: int,\n",
    "    convergence_threshold: float,\n",
    "    convergence_min_steps: int,\n",
    "    seed: int,\n",
    ") -> BaseAgent:\n",
    "    return GreedyAgent(\n",
    "        name=greedy_average.__name__,\n",
    "        env=env,\n",
    "        greedy_algorithm=greedy_average,\n",
    "        epsilon_config=epsilon_config,\n",
    "        optimistic_init=optimistic_init,\n",
    "        optimistic_times=optimistic_times,\n",
    "        convergence_threshold=convergence_threshold,\n",
    "        convergence_min_steps=convergence_min_steps,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_epsilon_agent(\n",
    "    env: RLEnv,\n",
    "    epsilon_config: EpsilonDecreasingConfig,\n",
    "    optimistic_init: bool,\n",
    "    optimistic_times: int,\n",
    "    convergence_threshold: float,\n",
    "    convergence_min_steps: int,\n",
    "    seed: int,\n",
    ") -> BaseAgent:\n",
    "    return GreedyAgent(\n",
    "        name=epsilon_average.__name__,\n",
    "        env=env,\n",
    "        greedy_algorithm=epsilon_average,\n",
    "        epsilon_config=epsilon_config,\n",
    "        optimistic_init=optimistic_init,\n",
    "        optimistic_times=optimistic_times,\n",
    "        convergence_threshold=convergence_threshold,\n",
    "        convergence_min_steps=convergence_min_steps,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_decreasing_agent(\n",
    "    env: RLEnv,\n",
    "    epsilon_config: EpsilonDecreasingConfig,\n",
    "    optimistic_init: bool,\n",
    "    optimistic_times: int,\n",
    "    convergence_threshold: float,\n",
    "    convergence_min_steps: int,\n",
    "    seed: int,\n",
    ") -> BaseAgent:\n",
    "    return GreedyAgent(\n",
    "        name=epsilon_decreasing_average.__name__,\n",
    "        env=env,\n",
    "        greedy_algorithm=epsilon_decreasing_average,\n",
    "        epsilon_config=epsilon_config,\n",
    "        optimistic_init=optimistic_init,\n",
    "        optimistic_times=optimistic_times,\n",
    "        convergence_threshold=convergence_threshold,\n",
    "        convergence_min_steps=convergence_min_steps,\n",
    "        seed=seed,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d41b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ucb1_agent(\n",
    "    env: RLEnv,\n",
    "    convergence_threshold: float,\n",
    "    convergence_min_steps: int,\n",
    "    seed: int,\n",
    ") -> BaseAgent:\n",
    "    return UCBAgent(\n",
    "        name=ucb1.__name__,\n",
    "        env=env,\n",
    "        ucb1_algorithm=ucb1,\n",
    "        convergence_threshold=convergence_threshold,\n",
    "        convergence_min_steps=convergence_min_steps,\n",
    "        seed=seed,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "079a4c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ts_agent(\n",
    "    env: RLEnv,\n",
    "    convergence_threshold: float,\n",
    "    convergence_min_steps: int,\n",
    "    seed: int,\n",
    ") -> BaseAgent:\n",
    "    return TSAgent(\n",
    "        name=TSAgent.__name__,\n",
    "        env=env,\n",
    "        convergence_threshold=convergence_threshold,\n",
    "        convergence_min_steps=convergence_min_steps,\n",
    "        seed=seed,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a0e0bb",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6993f9fc",
   "metadata": {},
   "source": [
    "## 普通贪婪算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f976af58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "达到收敛时的步数: 190\n",
      "达到收敛时的步数: 510\n",
      "达到收敛时的步数: 250\n",
      "达到收敛时的步数: 130\n",
      "达到收敛时的步数: 560\n",
      "达到收敛时的步数: 150\n",
      "达到收敛时的步数: 160\n",
      "达到收敛时的步数: 170\n",
      "达到收敛时的步数: 150\n",
      "达到收敛时的步数: 200\n",
      "达到收敛时的步数: 250\n",
      "达到收敛时的步数: 180\n",
      "达到收敛时的步数: 440\n",
      "达到收敛时的步数: 140\n",
      "达到收敛时的步数: 190\n",
      "达到收敛时的步数: 190\n",
      "达到收敛时的步数: 1190\n",
      "达到收敛时的步数: 390\n",
      "达到收敛时的步数: 140\n",
      "达到收敛时的步数: 190\n",
      "达到收敛时的步数: 130\n",
      "达到收敛时的步数: 210\n",
      "达到收敛时的步数: 300\n",
      "达到收敛时的步数: 210\n",
      "达到收敛时的步数: 520\n",
      "达到收敛时的步数: 210\n",
      "达到收敛时的步数: 140\n",
      "达到收敛时的步数: 520\n",
      "达到收敛时的步数: 130\n",
      "达到收敛时的步数: 270\n",
      "达到收敛时的步数: 210\n",
      "达到收敛时的步数: 160\n",
      "达到收敛时的步数: 480\n",
      "达到收敛时的步数: 410\n",
      "avg_regret=3996.850909090912 avg_regret_rate=0.04396536000000002 avg_total_reward=86912.24 avg_optimal_rate=0.6798143999999999 avg_convergence_steps=193.4 avg_convergence_rate=0.68\n",
      "values=[0.5, 8725.78, 2.62, 0.72, 0.26, 1.34, 16376.84, 61803.06, 0.14, 0.98] counts=[1.54, 12002.72, 4.06, 1.74, 1.26, 2.46, 20001.64, 67981.44, 1.14, 2.0]\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/greedy_average_1757851137.596528_T=100000_K=10_Q_0=1.png\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/greedy_average_1757851137.596528_T=100000_K=10_Q_0=1_x_log.png\n",
      "✅ 实验结果数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/greedy_average_1757851137.596528_T=100000_K=10_Q_0=1.json\n",
      "✅ 过程数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/greedy_average_1757851137.596528_T=100000_K=10_Q_0=1process.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id = get_run_id(greedy_average.__name__)\n",
    "file_name: Path = (\n",
    "    EXPERIMENT_DATA_DIR\n",
    "    / f\"{run_id}_T={STEPS}_K={MACHINE_COUNT}_Q_0={OPTIMISTIC_TIMES}.png\"\n",
    ")\n",
    "process_logger = ProcessDataLogger(\n",
    "    run_id=run_id,\n",
    "    total_steps=STEPS,\n",
    "    grid_size=GRID_SIZE,\n",
    ")\n",
    "\n",
    "agents, reward, metrics = batch_train(\n",
    "    count=RUN_COUNT,\n",
    "    agent_factory=create_greedy_agent,\n",
    "    env=ENV,\n",
    "    epsilon_config=EPSILON_CONFIG,\n",
    "    steps=STEPS,\n",
    "    seed=SEED,\n",
    "    optimistic_init=ENABLE_OPTIMISTIC,\n",
    "    optimistic_times=OPTIMISTIC_TIMES,\n",
    "    convergence_threshold=CONVERGENCE_THRESHOLD,\n",
    "    convergence_min_steps=CONVERGENCE_MIN_STEPS,\n",
    "    process_logger=process_logger,\n",
    ")\n",
    "print(metrics)\n",
    "print(reward)\n",
    "\n",
    "plot_metrics_history(agents, \"贪婪算法\", file_name, x_log=False)\n",
    "plot_metrics_history(agents, \"贪婪算法\", file_name, x_log=True)\n",
    "save_experiment_data(reward, metrics, file_name)\n",
    "process_logger.save(file_name.with_stem(file_name.stem + \"process\"), total_steps=STEPS)\n",
    "dump = process_logger.export(total_steps=STEPS)\n",
    "keys = list(dump.points[0].data.keys())\n",
    "\n",
    "del agents, reward, metrics, process_logger, dump\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d0fa0",
   "metadata": {},
   "source": [
    "## UCB1算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "854e802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "达到收敛时的步数: 20550\n",
      "达到收敛时的步数: 25740\n",
      "达到收敛时的步数: 24000\n",
      "达到收敛时的步数: 24400\n",
      "达到收敛时的步数: 24230\n",
      "达到收敛时的步数: 33740\n",
      "达到收敛时的步数: 24630\n",
      "达到收敛时的步数: 29670\n",
      "达到收敛时的步数: 26380\n",
      "达到收敛时的步数: 19370\n",
      "达到收敛时的步数: 23990\n",
      "达到收敛时的步数: 23520\n",
      "达到收敛时的步数: 25750\n",
      "达到收敛时的步数: 22280\n",
      "达到收敛时的步数: 27480\n",
      "达到收敛时的步数: 25100\n",
      "达到收敛时的步数: 22840\n",
      "达到收敛时的步数: 24090\n",
      "达到收敛时的步数: 24890\n",
      "达到收敛时的步数: 24650\n",
      "达到收敛时的步数: 20870\n",
      "达到收敛时的步数: 24570\n",
      "达到收敛时的步数: 25730\n",
      "达到收敛时的步数: 22000\n",
      "达到收敛时的步数: 24630\n",
      "达到收敛时的步数: 26310\n",
      "达到收敛时的步数: 27790\n",
      "达到收敛时的步数: 30100\n",
      "达到收敛时的步数: 31580\n",
      "达到收敛时的步数: 20090\n",
      "达到收敛时的步数: 27700\n",
      "达到收敛时的步数: 20480\n",
      "达到收敛时的步数: 19880\n",
      "达到收敛时的步数: 20070\n",
      "达到收敛时的步数: 25670\n",
      "达到收敛时的步数: 22890\n",
      "达到收敛时的步数: 24420\n",
      "达到收敛时的步数: 19100\n",
      "达到收敛时的步数: 24310\n",
      "达到收敛时的步数: 23840\n",
      "达到收敛时的步数: 22450\n",
      "达到收敛时的步数: 20000\n",
      "达到收敛时的步数: 23020\n",
      "达到收敛时的步数: 21860\n",
      "达到收敛时的步数: 26010\n",
      "达到收敛时的步数: 23310\n",
      "达到收敛时的步数: 28860\n",
      "达到收敛时的步数: 19730\n",
      "达到收敛时的步数: 25060\n",
      "达到收敛时的步数: 27830\n",
      "avg_regret=609.3109090909118 avg_regret_rate=0.00670242000000003 avg_total_reward=90299.78 avg_optimal_rate=0.9663803999999999 avg_convergence_steps=24349.2 avg_convergence_rate=1.0\n",
      "values=[13.58, 436.54, 172.88, 25.88, 6.96, 86.0, 1661.92, 87847.72, 2.74, 45.56] counts=[52.4, 599.22, 272.56, 72.14, 40.94, 158.48, 2031.38, 96638.04, 33.2, 101.64]\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/ucb1_1757851149.591326_T=100000_K=10_Q_0=1.png\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/ucb1_1757851149.591326_T=100000_K=10_Q_0=1_x_log.png\n",
      "✅ 实验结果数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/ucb1_1757851149.591326_T=100000_K=10_Q_0=1.json\n",
      "✅ 过程数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/ucb1_1757851149.591326_T=100000_K=10_Q_0=1process.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id = get_run_id(ucb1.__name__)\n",
    "file_name: Path = (\n",
    "    EXPERIMENT_DATA_DIR\n",
    "    / f\"{run_id}_T={STEPS}_K={MACHINE_COUNT}_Q_0={OPTIMISTIC_TIMES}.png\"\n",
    ")\n",
    "process_logger = ProcessDataLogger(\n",
    "    run_id=run_id,\n",
    "    total_steps=STEPS,\n",
    "    grid_size=GRID_SIZE,\n",
    ")\n",
    "\n",
    "agents, reward, metrics = batch_train(\n",
    "    count=RUN_COUNT,\n",
    "    agent_factory=create_ucb1_agent,\n",
    "    env=ENV,\n",
    "    steps=STEPS,\n",
    "    seed=SEED,\n",
    "    convergence_threshold=CONVERGENCE_THRESHOLD,\n",
    "    convergence_min_steps=CONVERGENCE_MIN_STEPS,\n",
    "    process_logger=process_logger,\n",
    ")\n",
    "print(metrics)\n",
    "print(reward)\n",
    "\n",
    "plot_metrics_history(agents, \"UCB1 算法\", file_name, x_log=False)\n",
    "plot_metrics_history(agents, \"UCB1 算法\", file_name, x_log=True)\n",
    "save_experiment_data(reward, metrics, file_name)\n",
    "process_logger.save(file_name.with_stem(file_name.stem + \"process\"), total_steps=STEPS)\n",
    "dump = process_logger.export(total_steps=STEPS)\n",
    "keys = list(dump.points[0].data.keys())\n",
    "\n",
    "del agents, reward, metrics, process_logger, dump\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9ffd59",
   "metadata": {},
   "source": [
    "# Thompson Sampling 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d294c600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "达到收敛时的步数: 2900\n",
      "达到收敛时的步数: 980\n",
      "达到收敛时的步数: 3950\n",
      "达到收敛时的步数: 710\n",
      "达到收敛时的步数: 610\n",
      "达到收敛时的步数: 1100\n",
      "达到收敛时的步数: 3640\n",
      "达到收敛时的步数: 2750\n",
      "达到收敛时的步数: 1430\n",
      "达到收敛时的步数: 1020\n",
      "达到收敛时的步数: 2010\n",
      "达到收敛时的步数: 790\n",
      "达到收敛时的步数: 1530\n",
      "达到收敛时的步数: 2460\n",
      "达到收敛时的步数: 520\n",
      "达到收敛时的步数: 2370\n",
      "达到收敛时的步数: 1230\n",
      "达到收敛时的步数: 3250\n",
      "达到收敛时的步数: 1840\n",
      "达到收敛时的步数: 460\n",
      "达到收敛时的步数: 1130\n",
      "达到收敛时的步数: 1420\n",
      "达到收敛时的步数: 1030\n",
      "达到收敛时的步数: 600\n",
      "达到收敛时的步数: 670\n",
      "达到收敛时的步数: 2330\n",
      "达到收敛时的步数: 610\n",
      "达到收敛时的步数: 2110\n",
      "达到收敛时的步数: 290\n",
      "达到收敛时的步数: 1240\n",
      "达到收敛时的步数: 1140\n",
      "达到收敛时的步数: 1050\n",
      "达到收敛时的步数: 1020\n",
      "达到收敛时的步数: 810\n",
      "达到收敛时的步数: 5240\n",
      "达到收敛时的步数: 1340\n",
      "达到收敛时的步数: 970\n",
      "达到收敛时的步数: 1520\n",
      "达到收敛时的步数: 2300\n",
      "达到收敛时的步数: 420\n",
      "达到收敛时的步数: 1600\n",
      "达到收敛时的步数: 2230\n",
      "达到收敛时的步数: 770\n",
      "达到收敛时的步数: 670\n",
      "达到收敛时的步数: 5620\n",
      "达到收敛时的步数: 450\n",
      "达到收敛时的步数: 1150\n",
      "达到收敛时的步数: 2670\n",
      "达到收敛时的步数: 390\n",
      "达到收敛时的步数: 930\n",
      "avg_regret=49.61090909091174 avg_regret_rate=0.0005457200000000289 avg_total_reward=90859.48 avg_optimal_rate=0.9971712000000003 avg_convergence_steps=1585.4 avg_convergence_rate=1.0\n",
      "values=[2.6, 32.76, 15.46, 3.24, 1.12, 8.28, 128.64, 90662.26, 0.42, 4.7] counts=[8.4, 45.6, 24.62, 9.16, 6.18, 15.76, 156.88, 99717.12, 5.14, 11.14]\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/thompson_sampling_1757851192.6531556_T=100000_K=10_Q_0=1.png\n",
      "✅ 字体文件 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/assets/微软雅黑.ttf 已加载\n",
      "✅ 图表已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/thompson_sampling_1757851192.6531556_T=100000_K=10_Q_0=1_x_log.png\n",
      "✅ 实验结果数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/thompson_sampling_1757851192.6531556_T=100000_K=10_Q_0=1.json\n",
      "✅ 过程数据已保存至 /home/Jese__Ki/Projects/learn/Python/rl_atomic/bandit/experiment_data/thompson_sampling_1757851192.6531556_T=100000_K=10_Q_0=1process.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "228748"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id = get_run_id(\"thompson_sampling\")\n",
    "file_name: Path = (\n",
    "    EXPERIMENT_DATA_DIR\n",
    "    / f\"{run_id}_T={STEPS}_K={MACHINE_COUNT}_Q_0={OPTIMISTIC_TIMES}.png\"\n",
    ")\n",
    "process_logger = ProcessDataLogger(\n",
    "    run_id=run_id,\n",
    "    total_steps=STEPS,\n",
    "    grid_size=GRID_SIZE,\n",
    ")\n",
    "\n",
    "agents, reward, metrics = batch_train(\n",
    "    count=RUN_COUNT,\n",
    "    agent_factory=create_ts_agent,\n",
    "    env=ENV,\n",
    "    steps=STEPS,\n",
    "    seed=SEED,\n",
    "    convergence_threshold=CONVERGENCE_THRESHOLD,\n",
    "    convergence_min_steps=CONVERGENCE_MIN_STEPS,\n",
    "    process_logger=process_logger,\n",
    ")\n",
    "print(metrics)\n",
    "print(reward)\n",
    "\n",
    "plot_metrics_history(agents, \"TS 算法\", file_name, x_log=False)\n",
    "plot_metrics_history(agents, \"TS 算法\", file_name, x_log=True)\n",
    "save_experiment_data(reward, metrics, file_name)\n",
    "process_logger.save(file_name.with_stem(file_name.stem + \"process\"), total_steps=STEPS)\n",
    "dump = process_logger.export(total_steps=STEPS)\n",
    "keys = list(dump.points[0].data.keys())\n",
    "\n",
    "del agents, reward, metrics, process_logger, dump\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
